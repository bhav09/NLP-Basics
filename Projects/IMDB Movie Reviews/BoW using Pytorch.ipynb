{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34173</th>\n",
       "      <td>This movie is brilliant. The comments made bef...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37937</th>\n",
       "      <td>I know not why people considered it trashy or ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2673</th>\n",
       "      <td>Growing up in the late 60s and 70s I could not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "34173  This movie is brilliant. The comments made bef...  positive\n",
       "37937  I know not why people considered it trashy or ...  positive\n",
       "2673   Growing up in the late 60s and 70s I could not...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_of_data='D:/pytorch-nlp-notebooks/IMDB Dataset.csv/IMDB Dataset.csv'\n",
    "pd.read_csv(path_of_data).sample(3) #here we print 3 rows randomly and see that the data is not encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "\n",
    "#dataset class will deal with loading and preprocessing of the dataset\n",
    "class dataset(Dataset):\n",
    "    def __init__(self,path):\n",
    "        corpus=[]\n",
    "        le=LabelEncoder()\n",
    "        df=pd.read_csv(path)\n",
    "        if type(df.sentiment[0])==str:\n",
    "            df.sentiment=le.fit_transform(df.sentiment)\n",
    "        for i in tqdm(range(len(df.review))):\n",
    "            text=df.review[i]\n",
    "            text=text.lower()\n",
    "            text=re.sub(r'[^a-zA-Z]{1,10}',' ',text)\n",
    "            corpus.append(text)\n",
    "        self.cv=CountVectorizer(stop_words='english', max_df=0.99, min_df=0.005)\n",
    "        self.x=self.cv.fit_transform(corpus).toarray()\n",
    "        self.labels=list(df.sentiment)\n",
    "        self.token2idx = self.cv.vocabulary_\n",
    "        #print('Token to index:',self.token2idx)\n",
    "        self.idx2token = {idx: token for token, idx in self.token2idx.items()}\n",
    "        print(self.idx2token)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i, :], self.labels[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 50000/50000 [00:07<00:00, 6320.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2083: 'reviewers', 1596: 'mentioned', 2771: 'watching', 1366: 'just', 798: 'episode', 1485: 'll', 1208: 'hooked', 2091: 'right', 824: 'exactly', 1134: 'happened', 273: 'br', 2554: 'thing', 2417: 'struck', 2161: 'scenes', 2730: 'violence', 2216: 'set', 2828: 'word', 2634: 'trust', 1161: 'hearted', 1959: 'pulls', 730: 'drugs', 2221: 'sex', 416: 'classic', 2691: 'use', 316: 'called', 1059: 'given', 2187: 'security', 2371: 'state', 975: 'focuses', 1534: 'mainly', 411: 'city', 2186: 'section', 1919: 'prison', 1063: 'glass', 869: 'face', 1181: 'high', 762: 'em', 1204: 'home', 1317: 'irish', 599: 'death', 899: 'far', 179: 'away', 2150: 'say', 1533: 'main', 117: 'appeal', 873: 'fact', 1067: 'goes', 2252: 'shows', 2841: 'wouldn', 581: 'dare', 993: 'forget', 1909: 'pretty', 1840: 'pictures', 1535: 'mainstream', 169: 'audiences', 378: 'charm', 2113: 'romance', 692: 'doesn', 1599: 'mess', 2149: 'saw', 1681: 'nasty', 2480: 'surreal', 517: 'couldn', 2001: 'ready', 2770: 'watched', 647: 'developed', 2507: 'taste', 1079: 'got', 1458: 'levels', 1088: 'graphic', 2306: 'sold', 1381: 'kill', 1763: 'order', 1607: 'middle', 415: 'class', 2641: 'turned', 1402: 'lack', 2409: 'street', 2284: 'skills', 846: 'experience', 2661: 'uncomfortable', 2724: 'viewing', 2546: 'thats', 2596: 'touch', 2822: 'wonderful', 1480: 'little', 1930: 'production', 942: 'filming', 2515: 'technique', 1751: 'old', 2577: 'time', 203: 'bbc', 902: 'fashion', 1060: 'gives', 2201: 'sense', 2003: 'realism', 793: 'entire', 1841: 'piece', 26: 'actors', 864: 'extremely', 401: 'chosen', 1604: 'michael', 2739: 'voices', 2633: 'truly', 751: 'editing', 2023: 'references', 2802: 'williams', 2838: 'worth', 2847: 'written', 1821: 'performed', 1092: 'great', 1564: 'master', 446: 'comedy', 1462: 'life', 2009: 'really', 447: 'comes', 2555: 'things', 898: 'fantasy', 1108: 'guard', 2606: 'traditional', 716: 'dream', 2516: 'techniques', 2042: 'remains', 2309: 'solid', 1860: 'plays', 1396: 'knowledge', 1797: 'particularly', 2217: 'sets', 961: 'flat', 2474: 'surface', 2536: 'terribly', 2562: 'thought', 2773: 'way', 2339: 'spend', 1221: 'hot', 2460: 'summer', 2785: 'weekend', 2279: 'sitting', 62: 'air', 2547: 'theater', 1464: 'light', 1864: 'plot', 654: 'dialogue', 2817: 'witty', 373: 'characters', 1467: 'likable', 2209: 'serial', 1383: 'killer', 675: 'disappointed', 2006: 'realize', 1566: 'match', 1868: 'point', 2098: 'risk', 1939: 'proof', 76: 'allen', 1023: 'fully', 499: 'control', 2432: 'style', 1105: 'grown', 1513: 'love', 1423: 'laughed', 445: 'comedies', 2853: 'years', 602: 'decade', 2708: 've', 1263: 'impressed', 1547: 'managed', 2587: 'tone', 2224: 'sexy', 1250: 'image', 174: 'average', 2856: 'young', 2818: 'woman', 333: 'career', 651: 'devil', 2782: 'wears', 1305: 'interesting', 1019: 'friends', 199: 'basically', 893: 'family', 270: 'boy', 2558: 'thinks', 2860: 'zombie', 1791: 'parents', 934: 'fighting', 1658: 'movie', 2302: 'soap', 1758: 'opera', 2448: 'suddenly', 607: 'decides', 1749: 'ok', 1068: 'going', 1538: 'make', 940: 'film', 605: 'decide', 2566: 'thriller', 710: 'drama', 2769: 'watchable', 1468: 'like', 2002: 'real', 2595: 'totally', 843: 'expected', 2263: 'similar', 1295: 'instead', 2354: 'spots', 1859: 'playing', 653: 'dialogs', 2247: 'shots', 1246: 'ignore', 1640: 'money', 2736: 'visually', 2429: 'stunning', 2768: 'watch', 1661: 'mr', 1745: 'offers', 1228: 'human', 2528: 'telling', 1891: 'power', 2441: 'success', 1814: 'people', 662: 'different', 2281: 'situations', 769: 'encounter', 136: 'arthur', 1855: 'play', 2551: 'theme', 671: 'director', 23: 'action', 1903: 'present', 1700: 'new', 2855: 'york', 1582: 'meet', 479: 'connected', 1824: 'person', 1394: 'know', 1910: 'previous', 488: 'contact', 2317: 'sophisticated', 1495: 'look', 2492: 'taken', 1481: 'live', 2834: 'world', 1052: 'gets', 1839: 'picture', 229: 'big', 226: 'best', 1846: 'place', 344: 'case', 22: 'acting', 1073: 'good', 669: 'direction', 2387: 'steve', 1368: 'kane', 2068: 'rest', 2497: 'talented', 347: 'cast', 442: 'come', 75: 'alive', 2812: 'wish', 1522: 'luck', 2830: 'work', 1921: 'probably', 910: 'favorite', 2402: 'story', 354: 'cause', 262: 'boring', 639: 'despite', 1152: 'having', 2192: 'seen', 2578: 'times', 1810: 'paul', 1819: 'performance', 289: 'brings', 2512: 'tears', 867: 'eyes', 588: 'davis', 2489: 'sympathetic', 2110: 'roles', 614: 'delight', 1380: 'kids', 2152: 'says', 719: 'dressed', 393: 'children', 1541: 'makes', 1024: 'fun', 1651: 'mother', 2295: 'slow', 1135: 'happening', 219: 'believable', 705: 'dozen', 2472: 'sure', 585: 'dated', 2210: 'series', 2584: 'today', 287: 'bring', 1378: 'kid', 831: 'excitement', 1097: 'grew', 238: 'black', 2797: 'white', 2644: 'tv', 1174: 'hero', 2784: 'week', 2740: 'vote', 2176: 'sea', 1235: 'hunt', 1694: 'need', 367: 'change', 1777: 'pace', 2772: 'water', 46: 'adventure', 1748: 'oh', 2543: 'thank', 2720: 'view', 1659: 'movies', 220: 'believe', 2751: 'wanna', 1702: 'nice', 1999: 'read', 1866: 'plus', 1870: 'points', 1475: 'lines', 1453: 'let', 1439: 'leave', 702: 'doubt', 1454: 'lets', 84: 'amazing', 1015: 'fresh', 1240: 'idea', 63: 'aired', 285: 'brilliant', 1027: 'funny', 112: 'anymore', 465: 'complete', 2765: 'waste', 886: 'fallen', 2846: 'writing', 1786: 'painfully', 185: 'bad', 1820: 'performances', 1609: 'mildly', 791: 'entertaining', 1111: 'guest', 1140: 'hard', 1125: 'hand', 1765: 'original', 400: 'chose', 190: 'band', 978: 'followed', 2016: 'recognize', 956: 'fit', 925: 'felt', 2366: 'stars', 2066: 'respect', 1227: 'huge', 181: 'awful', 1886: 'positive', 453: 'comments', 1497: 'looking', 1002: 'forward', 1630: 'mistake', 945: 'films', 2837: 'worst', 1779: 'pacing', 2403: 'storyline', 2325: 'soundtrack', 2314: 'song', 1410: 'lame', 520: 'country', 2638: 'tune', 1856: 'played', 1498: 'looks', 383: 'cheap', 863: 'extreme', 1990: 'rarely', 1139: 'happy', 771: 'end', 544: 'credits', 1061: 'giving', 2166: 'score', 1543: 'making', 234: 'bit', 757: 'effort', 1426: 'laughter', 1168: 'hell', 1637: 'mom', 1469: 'liked', 322: 'camp', 72: 'alien', 1976: 'quirky', 1233: 'humour', 197: 'based', 30: 'actual', 1740: 'odd', 657: 'didn', 1352: 'jokes', 1520: 'low', 302: 'budget', 1922: 'problem', 817: 'eventually', 1506: 'lost', 1255: 'imagine', 228: 'better', 2636: 'try', 294: 'brother', 1852: 'planet', 319: 'came', 2012: 'recall', 2159: 'scene', 746: 'eating', 1592: 'men', 1217: 'horror', 387: 'cheesy', 2144: 'saturday', 2581: 'tired', 999: 'formula', 1641: 'monster', 2649: 'type', 2697: 'usually', 1267: 'included', 207: 'beautiful', 586: 'daughter', 1933: 'professor', 659: 'died', 332: 'care', 2114: 'romantic', 96: 'angle', 2852: 'year', 1897: 'predictable', 1865: 'plots', 1231: 'humor', 1419: 'later', 1954: 'psycho', 1514: 'loved', 2363: 'star', 740: 'early', 2141: 'sat', 2590: 'took', 1721: 'notice', 2157: 'scary', 1887: 'possible', 2125: 'rules', 895: 'fan', 784: 'enjoyed', 1573: 'maybe', 116: 'apparently', 267: 'bought', 2092: 'rights', 1493: 'long', 58: 'ago', 1031: 'game', 1385: 'killing', 2185: 'secret', 2065: 'research', 1321: 'island', 2759: 'warned', 919: 'feeling', 1858: 'players', 1678: 'names', 31: 'actually', 2495: 'tale', 1327: 'jack', 2854: 'yes', 1050: 'german', 2632: 'true', 1829: 'perspective', 698: 'don', 1496: 'looked', 821: 'evil', 1528: 'mad', 2165: 'scientist', 706: 'dr', 2308: 'soldiers', 2049: 'reminds', 2348: 'spoiler', 2010: 'reason', 2086: 'rich', 2827: 'woods', 1151: 'haven', 1071: 'gone', 2368: 'started', 2376: 'stay', 618: 'delivers', 1575: 'meaning', 2444: 'suck', 656: 'did', 251: 'boat', 1421: 'laugh', 60: 'ahead', 106: 'annoying', 2239: 'shoot', 1622: 'minutes', 2170: 'screen', 2226: 'shakespeare', 125: 'appreciate', 2637: 'trying', 2122: 'ruin', 361: 'certain', 2624: 'tried', 802: 'era', 2829: 'words', 1816: 'perfection', 2843: 'write', 2541: 'text', 781: 'english', 2151: 'saying', 691: 'does', 566: 'cut', 897: 'fantastic', 894: 'famous', 1049: 'george', 2111: 'roll', 1545: 'man', 486: 'constant', 818: 'everybody', 1388: 'kind', 714: 'drawn', 804: 'erotic', 82: 'amateurish', 2659: 'unbelievable', 235: 'bits', 2319: 'sort', 2162: 'school', 1936: 'project', 2557: 'thinking', 2391: 'stock', 237: 'bizarre', 2470: 'supposed', 2602: 'town', 1314: 'involved', 1452: 'lessons', 1436: 'learned', 1977: 'quite', 2089: 'ridiculous', 1508: 'lots', 2285: 'skin', 1712: 'nonsense', 2033: 'relationship', 770: 'encounters', 5: 'absurd', 575: 'dance', 1801: 'pass', 1613: 'million', 1610: 'miles', 2766: 'wasted', 2342: 'spent', 51: 'africa', 2266: 'simply', 878: 'fails', 328: 'capture', 2538: 'terror', 2582: 'title', 827: 'excellent', 1199: 'holds', 828: 'exception', 2804: 'wilson', 918: 'feel', 372: 'character', 1536: 'major', 908: 'fault', 2710: 'version', 1328: 'jackson', 163: 'attempts', 739: 'earlier', 2334: 'special', 756: 'effects', 782: 'enjoy', 1752: 'older', 1214: 'horrible', 2764: 'wasn', 1621: 'minute', 933: 'fight', 366: 'chance', 648: 'development', 309: 'busy', 2127: 'running', 2488: 'sword', 766: 'emotional', 1527: 'machine', 2753: 'wanted', 640: 'destroy', 2392: 'stolen', 2762: 'wars', 826: 'examples', 1054: 'ghost', 946: 'final', 214: 'beginning', 158: 'attacked', 2072: 'return', 2825: 'wood', 2714: 'victim', 2742: 'wait', 2741: 'vs', 1230: 'humans', 2527: 'tell', 1688: 'nazi', 1473: 'line', 2129: 'rushed', 473: 'conclusion', 43: 'adult', 677: 'disappointment', 2145: 'save', 2045: 'remember', 407: 'cinema', 582: 'dark', 1848: 'places', 571: 'dad', 2274: 'sister', 780: 'england', 120: 'appearance', 25: 'actor', 574: 'dan', 2556: 'think', 2246: 'shot', 660: 'dies', 1398: 'knows', 737: 'dvd', 430: 'club', 2228: 'shame', 1157: 'hear', 1709: 'nominated', 1070: 'golden', 926: 'female', 875: 'facts', 1817: 'perfectly', 951: 'fine', 137: 'artist', 735: 'dull', 2174: 'script', 2469: 'suppose', 2791: 'weren', 1676: 'naked', 2459: 'summary', 2146: 'saved', 521: 'couple', 1224: 'hours', 658: 'die', 2206: 'sequels', 2475: 'surprise', 2848: 'wrong', 1117: 'guy', 470: 'concept', 1653: 'mountain', 2064: 'rescue', 2396: 'stop', 807: 'especially', 850: 'expert', 196: 'base', 1362: 'jumping', 872: 'facial', 858: 'expressions', 1843: 'pile', 1365: 'junk', 21: 'acted', 1218: 'horse', 1512: 'lovable', 1863: 'plenty', 1425: 'laughs', 1347: 'john', 269: 'box', 56: 'agent', 1173: 'henry', 550: 'cringe', 2840: 'worthy', 487: 'constantly', 1785: 'painful', 679: 'disbelief', 1237: 'hurt', 111: 'anybody', 2796: 'whilst', 2473: 'surely', 1008: 'frank', 1056: 'girl', 1595: 'mention', 1491: 'london', 2695: 'using', 985: 'football', 1169: 'help', 1695: 'needed', 335: 'cares', 1133: 'happen', 1521: 'lower', 842: 'expectations', 2640: 'turn', 2781: 'wearing', 2535: 'terrible', 2248: 'shouldn', 150: 'aspects', 1789: 'paper', 2208: 'sequences', 884: 'fake', 1781: 'packed', 530: 'crappy', 1474: 'liners', 90: 'amusing', 2819: 'women', 2699: 'utterly', 119: 'appear', 2788: 'weird', 516: 'costumes', 876: 'fail', 1625: 'miserably', 4: 'absolutely', 2820: 'won', 2029: 'regret', 338: 'carries', 1322: 'isn', 1671: 'music', 2493: 'takes', 1103: 'grow', 1158: 'heard', 2367: 'start', 2405: 'straight', 1569: 'matter', 2692: 'used', 2338: 'speed', 1186: 'hilarious', 1507: 'lot', 1660: 'moving', 2696: 'usual', 2447: 'sudden', 102: 'animation', 2836: 'worse', 306: 'bunch', 2126: 'run', 39: 'addition', 2721: 'viewed', 2504: 'tape', 2713: 'vhs', 1857: 'player', 1373: 'kept', 1608: 'mike', 1664: 'mst', 1836: 'pick', 2180: 'season', 1206: 'honestly', 195: 'barely', 2482: 'survive', 1166: 'heck', 924: 'fellow', 1124: 'halloween', 1705: 'night', 2430: 'stupid', 1767: 'originally', 15: 'according', 2153: 'scale', 1577: 'means', 2680: 'unless', 2013: 'received', 1397: 'known', 1794: 'parker', 27: 'actress', 78: 'allowed', 1358: 'judge', 259: 'bore', 627: 'depth', 507: 'cop', 921: 'feels', 2717: 'video', 2360: 'standards', 2268: 'singer', 467: 'complex', 17: 'accurate', 643: 'details', 1884: 'portrays', 1675: 'naive', 736: 'dumb', 2109: 'role', 1827: 'personality', 2537: 'terrific', 1344: 'job', 2315: 'songs', 2671: 'unfortunately', 1994: 'ratings', 2690: 'usa', 519: 'countries', 357: 'cell', 1565: 'masterpiece', 2627: 'trip', 1615: 'mind', 820: 'evidence', 19: 'achieved', 217: 'beings', 143: 'aside', 2563: 'thoughts', 885: 'fall', 536: 'creating', 1532: 'magnificent', 584: 'date', 1730: 'numerous', 2436: 'substance', 1876: 'poorly', 1626: 'miss', 2605: 'tradition', 1028: 'future', 1211: 'hopefully', 977: 'follow', 1755: 'opened', 700: 'door', 1253: 'imagination', 2752: 'want', 1828: 'personally', 2789: 'welcome', 365: 'challenge', 2500: 'talk', 1289: 'inside', 59: 'agree', 1046: 'genre', 1384: 'killers', 2831: 'worked', 877: 'failed', 2646: 'twist', 1835: 'physically', 1905: 'presented', 901: 'fascinating', 1355: 'journey', 1673: 'mysterious', 2435: 'subject', 851: 'explain', 1337: 'jennifer', 788: 'enter', 274: 'brain', 1461: 'lies', 2491: 'table', 2755: 'wants', 1109: 'guess', 1101: 'ground', 1722: 'noticed', 2249: 'showed', 2005: 'reality', 497: 'contrast', 284: 'bright', 2737: 'visuals', 1711: 'nonetheless', 634: 'design', 2476: 'surprised', 409: 'cinematography', 2745: 'walk', 1754: 'open', 865: 'eye', 532: 'crazy', 1441: 'leaving', 168: 'audience', 1587: 'member', 2301: 'smoking', 1872: 'political', 2142: 'satire', 1202: 'hollywood', 242: 'blockbuster', 2702: 'values', 1476: 'list', 2642: 'turning', 2344: 'spirit', 2756: 'war', 838: 'existence', 1875: 'poor', 1796: 'particular', 1047: 'genuine', 1901: 'prepared', 2: 'able', 2358: 'stand', 87: 'america', 1026: 'funniest', 2718: 'videos', 573: 'damn', 1148: 'hated', 2651: 'typical', 2331: 'speak', 2067: 'responsible', 1132: 'hanging', 576: 'dancing', 422: 'clich', 475: 'conflict', 594: 'dealing', 1414: 'large', 462: 'compelling', 2262: 'silver', 412: 'civil', 433: 'cold', 2365: 'starring', 1428: 'law', 317: 'calling', 794: 'entirely', 1757: 'opens', 1479: 'literally', 1973: 'quick', 673: 'dirty', 202: 'battle', 2207: 'sequence', 1965: 'puts', 1065: 'glory', 752: 'edward', 1823: 'period', 2307: 'soldier', 1107: 'gruesome', 1715: 'north', 801: 'equally', 2328: 'southern', 1444: 'left', 124: 'appears', 2490: 'sympathy', 2035: 'relatively', 2312: 'somewhat', 498: 'contrived', 2316: 'soon', 2643: 'turns', 2148: 'saving', 477: 'confusing', 2158: 'scenario', 2754: 'wanting', 215: 'begins', 2775: 'ways', 1947: 'prove', 450: 'coming', 523: 'course', 2600: 'tough', 1172: 'helps', 1261: 'importantly', 296: 'brought', 2219: 'settings', 688: 'disturbing', 2592: 'torn', 2327: 'south', 2479: 'surprisingly', 907: 'father', 1996: 'ray', 611: 'deeply', 2630: 'troubled', 1095: 'greatly', 368: 'changed', 110: 'anti', 1600: 'message', 754: 'effective', 1150: 'haunting', 1411: 'land', 2779: 'weapons', 1805: 'past', 360: 'century', 753: 'effect', 109: 'anthony', 1549: 'manages', 1645: 'mood', 154: 'atmosphere', 423: 'climax', 1367: 'justice', 2823: 'wonderfully', 1687: 'nature', 957: 'fits', 2675: 'unique', 2733: 'vision', 789: 'entertain', 1483: 'lives', 113: 'apart', 637: 'desperate', 1098: 'gripping', 2485: 'suspense', 2682: 'unlikely', 1708: 'noir', 2763: 'washington', 458: 'company', 2408: 'stranger', 114: 'apartment', 2103: 'robert', 2132: 'ryan', 731: 'drunk', 1220: 'host', 2138: 'sam', 1136: 'happens', 1340: 'jewish', 1871: 'police', 644: 'detective', 888: 'falls', 2183: 'second', 301: 'buddy', 2259: 'silence', 2465: 'superior', 1686: 'naturally', 98: 'angry', 1544: 'male', 359: 'central', 2298: 'smart', 2133: 'sad', 1372: 'kelly', 2297: 'small', 1589: 'memorable', 1129: 'hands', 160: 'attempt', 854: 'explanation', 1723: 'novel', 2087: 'richard', 596: 'dealt', 1039: 'gay', 1667: 'murder', 1701: 'news', 812: 'europe', 2426: 'studios', 1890: 'potential', 2833: 'works', 1042: 'general', 1057: 'girlfriend', 1971: 'question', 952: 'finest', 1274: 'indian', 667: 'directed', 2442: 'successful', 1196: 'hitting', 454: 'commercial', 1291: 'inspiration', 1433: 'leading', 672: 'directors', 1273: 'india', 2529: 'tells', 2681: 'unlike', 535: 'creates', 127: 'approach', 796: 'environment', 2707: 'various', 1144: 'harsh', 2452: 'suffers', 547: 'crime', 758: 'efforts', 1434: 'leads', 18: 'achieve', 95: 'anger', 935: 'fights', 178: 'aware', 1698: 'negative', 948: 'finally', 2561: 'thoroughly', 510: 'core', 278: 'breaks', 1034: 'gangster', 1225: 'house', 2243: 'short', 501: 'conversation', 1120: 'hair', 1639: 'moments', 1960: 'punch', 1281: 'influence', 522: 'courage', 286: 'brilliantly', 223: 'belongs', 1883: 'portraying', 768: 'emotions', 830: 'excited', 559: 'cult', 417: 'classics', 183: 'baby', 1917: 'print', 1968: 'quality', 1179: 'hide', 592: 'deadly', 2568: 'thrilling', 1756: 'opening', 823: 'exact', 2080: 'revenge', 1652: 'motion', 2656: 'ultimately', 38: 'adding', 1296: 'insult', 733: 'dubbed', 2258: 'significant', 1122: 'halfway', 1943: 'protagonist', 1710: 'non', 55: 'aged', 1326: 'italy', 1312: 'introduced', 1484: 'living', 1222: 'hotel', 1325: 'italian', 2332: 'speaking', 308: 'business', 191: 'bank', 1714: 'normally', 518: 'count', 346: 'cash', 697: 'dollars', 1628: 'missing', 1548: 'manager', 425: 'close', 16: 'account', 913: 'fear', 353: 'caught', 123: 'appearing', 2635: 'truth', 2585: 'told', 12: 'accepted', 481: 'consequences', 1284: 'initially', 1121: 'half', 628: 'described', 2075: 'reveal', 590: 'days', 1707: 'nights', 1741: 'oddly', 2077: 'revealing', 2191: 'seemingly', 2683: 'unnecessary', 809: 'essential', 1243: 'identity', 1519: 'loving', 750: 'edited', 2395: 'stood', 2616: 'treat', 2036: 'release', 759: 'element', 65: 'al', 2694: 'uses', 474: 'condition', 610: 'deeper', 609: 'deep', 1262: 'impossible', 1085: 'grand', 1446: 'legendary', 1591: 'memory', 1017: 'friend', 1145: 'hasn', 1950: 'provide', 2580: 'tiny', 2807: 'window', 2270: 'single', 1975: 'quiet', 2494: 'taking', 2743: 'waiting', 1925: 'produce', 900: 'fare', 1401: 'la', 1505: 'loss', 844: 'expecting', 1780: 'pack', 841: 'expect', 1738: 'occasional', 153: 'assume', 303: 'build', 2532: 'tension', 2519: 'tedious', 904: 'fast', 1038: 'gave', 2211: 'seriously', 1431: 'lead', 1072: 'gonna', 471: 'concerned', 2496: 'talent', 362: 'certainly', 175: 'avoid', 260: 'bored', 1787: 'paint', 732: 'dry', 2323: 'sounded', 567: 'cute', 1619: 'mini', 2487: 'sweet', 54: 'age', 1275: 'indie', 749: 'edge', 2821: 'wonder', 1141: 'hardly', 1413: 'language', 1727: 'nudity', 1213: 'hoping', 2040: 'religious', 1477: 'listen', 1339: 'jesus', 2780: 'wear', 2526: 'television', 1831: 'pg', 1992: 'rated', 2025: 'refuses', 2053: 'rent', 2455: 'suicide', 1697: 'needs', 1882: 'portrayed', 1753: 'ones', 2523: 'teenagers', 1962: 'purely', 149: 'aspect', 356: 'causes', 1728: 'number', 2524: 'teens', 1269: 'including', 213: 'begin', 1176: 'heroine', 534: 'created', 2407: 'strangely', 701: 'double', 2214: 'serves', 1963: 'purpose', 1078: 'gory', 2454: 'suggests', 129: 'area', 115: 'apparent', 36: 'add', 20: 'act', 2803: 'willing', 466: 'completely', 1726: 'nude', 1209: 'hope', 979: 'following', 2654: 'uk', 562: 'curiosity', 2701: 'value', 2835: 'worry', 1737: 'obviously', 2419: 'struggle', 936: 'figure', 2778: 'weapon', 1750: 'okay', 587: 'david', 2088: 'ride', 1094: 'greatest', 2844: 'writer', 1163: 'heaven', 2121: 'rubbish', 1238: 'husband', 1929: 'product', 849: 'experiment', 469: 'computer', 1935: 'program', 1940: 'propaganda', 2571: 'thrown', 1949: 'proves', 2379: 'steal', 1945: 'protect', 24: 'actions', 314: 'cage', 2240: 'shooting', 431: 'clue', 420: 'clearly', 2253: 'shut', 321: 'camera', 391: 'child', 1537: 'majority', 545: 'creepy', 2790: 'went', 348: 'casting', 1633: 'mixed', 1197: 'hold', 1330: 'james', 1357: 'jr', 1617: 'mindless', 792: 'entertainment', 2017: 'recommend', 257: 'book', 1324: 'issues', 1979: 'race', 2591: 'topic', 1825: 'personal', 848: 'experiences', 1104: 'growing', 395: 'china', 184: 'background', 1399: 'kong', 2386: 'stereotypical', 2046: 'remembered', 1353: 'jones', 1768: 'oscar', 689: 'doctor', 2801: 'william', 1718: 'notch', 388: 'chemistry', 1517: 'lovers', 1952: 'provides', 1162: 'hearts', 774: 'ending', 482: 'consider', 2400: 'stories', 825: 'example', 1490: 'logic', 496: 'continuity', 369: 'changes', 652: 'dialog', 1784: 'pain', 1036: 'gary', 1486: 'local', 2609: 'trailer', 476: 'confused', 508: 'cops', 2184: 'seconds', 1114: 'gun', 1382: 'killed', 726: 'drives', 1193: 'hit', 1167: 'held', 2373: 'states', 1083: 'grace', 1066: 'god', 133: 'army', 943: 'filmmaker', 212: 'began', 2370: 'starts', 1185: 'highly', 2670: 'unfortunate', 816: 'events', 840: 'exists', 1457: 'level', 1422: 'laughable', 2505: 'target', 2213: 'served', 1629: 'mission', 1033: 'gang', 315: 'california', 490: 'contains', 173: 'available', 2203: 'sent', 2361: 'standing', 1242: 'identify', 2832: 'working', 937: 'figured', 88: 'american', 331: 'car', 1720: 'noted', 613: 'degree', 1084: 'grade', 1223: 'hour', 2047: 'remind', 89: 'americans', 2137: 'sake', 1207: 'honor', 1165: 'heavy', 1672: 'musical', 703: 'douglas', 1799: 'parts', 2839: 'worthwhile', 460: 'compared', 2269: 'singing', 230: 'bigger', 1465: 'lighting', 617: 'delivered', 456: 'common', 1759: 'opinion', 1736: 'obvious', 568: 'cuts', 2502: 'talking', 783: 'enjoyable', 2468: 'supporting', 2085: 'revolves', 1559: 'mary', 2565: 'thousands', 263: 'born', 741: 'earth', 100: 'animals', 2428: 'stuff', 2462: 'sunday', 1956: 'public', 1690: 'nearly', 2559: 'thirty', 1418: 'late', 2172: 'screenplay', 1256: 'imdb', 2278: 'site', 376: 'charles', 2483: 'susan', 882: 'faith', 2078: 'reveals', 2280: 'situation', 1432: 'leader', 2586: 'tom', 2117: 'rose', 1154: 'head', 1649: 'morgan', 2799: 'wife', 2140: 'sarah', 664: 'dimensional', 264: 'boss', 2738: 'voice', 2193: 'sees', 964: 'flaws', 2725: 'views', 1961: 'pure', 2545: 'thanks', 1647: 'moore', 2607: 'tragedy', 1788: 'pair', 1555: 'marry', 513: 'cost', 1503: 'loses', 2857: 'younger', 1438: 'learns', 216: 'behavior', 2236: 'shock', 1906: 'presents', 2313: 'son', 683: 'discovery', 2594: 'total', 1363: 'jumps', 1719: 'note', 250: 'board', 1853: 'plans', 2099: 'rival', 2610: 'train', 2777: 'wealthy', 1991: 'rate', 2303: 'social', 452: 'commentary', 1713: 'normal', 2598: 'touches', 1691: 'neat', 2826: 'wooden', 947: 'finale', 49: 'affair', 1318: 'ironic', 2542: 'th', 1022: 'fu', 2794: 'westerns', 1075: 'gordon', 2793: 'western', 927: 'fest', 2265: 'simple', 29: 'acts', 1680: 'narrative', 1556: 'martial', 140: 'arts', 1900: 'premise', 2726: 'village', 855: 'exploitation', 1620: 'minor', 1053: 'getting', 541: 'credibility', 1634: 'mob', 1435: 'learn', 2288: 'slapstick', 1389: 'kinda', 2411: 'strength', 2322: 'sound', 1258: 'impact', 2618: 'treatment', 2024: 'refreshing', 2798: 'wide', 1183: 'highlight', 97: 'angles', 1657: 'moves', 32: 'ad', 967: 'flicks', 2057: 'repeat', 2171: 'screening', 1412: 'lane', 2349: 'spoilers', 891: 'familiar', 1287: 'innocent', 1342: 'jimmy', 2058: 'repeated', 987: 'forced', 132: 'arms', 1654: 'mouth', 1010: 'fred', 773: 'ended', 1006: 'france', 406: 'church', 1984: 'raised', 1219: 'hospital', 923: 'fell', 1731: 'nurse', 1523: 'lucky', 1733: 'obnoxious', 1554: 'married', 1013: 'french', 1553: 'marriage', 236: 'bitter', 13: 'accident', 1985: 'ran', 2410: 'streets', 681: 'discovered', 2731: 'violent', 1393: 'knew', 813: 'european', 1: 'ability', 589: 'day', 1562: 'massacre', 1588: 'members', 1159: 'hearing', 292: 'broke', 2188: 'seeing', 1160: 'heart', 37: 'added', 606: 'decided', 1456: 'letting', 122: 'appeared', 1386: 'kills', 253: 'bodies', 2061: 'reporter', 680: 'discover', 600: 'deaths', 1650: 'morning', 1670: 'murders', 591: 'dead', 2244: 'shortly', 2291: 'sleep', 896: 'fans', 1993: 'rating', 1790: 'par', 871: 'faces', 146: 'asking', 61: 'ain', 2329: 'space', 2467: 'support', 1889: 'post', 631: 'deserve', 2251: 'shown', 2362: 'stands', 1149: 'haunted', 835: 'execution', 2195: 'self', 1170: 'helped', 533: 'create', 2071: 'retarded', 248: 'blows', 776: 'ends', 663: 'difficult', 690: 'documentary', 2283: 'skill', 970: 'flow', 64: 'aka', 1551: 'mark', 2722: 'viewer', 880: 'fair', 1923: 'problems', 853: 'explains', 862: 'extras', 1879: 'porn', 1248: 'ii', 1502: 'lose', 1102: 'group', 1683: 'national', 2611: 'training', 630: 'desert', 1268: 'includes', 244: 'blood', 1116: 'guts', 972: 'flying', 1988: 'rape', 707: 'drag', 1076: 'gore', 1361: 'jump', 2378: 'stays', 604: 'decent', 1440: 'leaves', 1746: 'office', 1771: 'outside', 2094: 'rings', 1463: 'lifetime', 1091: 'grave', 2521: 'teenage', 1194: 'hitchcock', 2552: 'themes', 290: 'british', 304: 'building', 1693: 'necessary', 2567: 'thrillers', 1395: 'knowing', 1927: 'producer', 668: 'directing', 760: 'elements', 2231: 'sharp', 2814: 'wit', 282: 'brief', 2073: 'returning', 1815: 'perfect', 1501: 'los', 1584: 'meets', 1266: 'include', 1878: 'popular', 1635: 'model', 1557: 'martin', 1379: 'kidnapped', 1632: 'mix', 997: 'form', 1699: 'network', 1406: 'ladies', 549: 'criminals', 1957: 'pull', 1115: 'guns', 1271: 'incredibly', 135: 'art', 779: 'engaging', 421: 'clever', 2648: 'twists', 2728: 'villains', 320: 'cameo', 729: 'drug', 2399: 'store', 2257: 'sign', 50: 'afraid', 814: 'evening', 69: 'albert', 2273: 'sir', 1966: 'putting', 1391: 'king', 724: 'driven', 2254: 'sick', 2357: 'stage', 505: 'convincing', 2533: 'term', 2514: 'technically', 1558: 'marvelous', 130: 'aren', 2451: 'suffering', 2501: 'talked', 7: 'academy', 177: 'awards', 1830: 'peter', 35: 'adapted', 790: 'entertained', 1899: 'pregnant', 715: 'dreadful', 1260: 'important', 1494: 'longer', 1369: 'kate', 444: 'comedic', 28: 'actresses', 1924: 'process', 162: 'attempting', 1449: 'lesbian', 2534: 'terms', 676: 'disappointing', 912: 'favourite', 1515: 'lovely', 1265: 'impressive', 1624: 'miscast', 1880: 'portray', 1648: 'moral', 2021: 'redeeming', 1967: 'qualities', 1677: 'named', 2383: 'stephen', 1983: 'raise', 2522: 'teenager', 2260: 'silent', 742: 'easily', 2043: 'remake', 709: 'drags', 2261: 'silly', 2593: 'torture', 437: 'colorful', 1171: 'helping', 345: 'cases', 986: 'force', 822: 'ex', 1913: 'priest', 1594: 'mentally', 2076: 'revealed', 489: 'contain', 903: 'fashioned', 1674: 'mystery', 2415: 'strong', 1110: 'guessing', 2018: 'recommended', 384: 'check', 2230: 'share', 2406: 'strange', 272: 'boys', 1567: 'material', 265: 'bother', 612: 'definitely', 209: 'beauty', 151: 'ass', 883: 'faithful', 2612: 'trapped', 645: 'determined', 1011: 'free', 1531: 'magical', 1040: 'gem', 14: 'accidentally', 969: 'floor', 2070: 'results', 540: 'creatures', 744: 'easy', 2394: 'stone', 2622: 'trick', 1516: 'lover', 326: 'capable', 2811: 'wise', 2320: 'sorts', 2629: 'trouble', 1341: 'jim', 327: 'captain', 1593: 'mental', 1808: 'patient', 529: 'crap', 1902: 'presence', 2198: 'semi', 2294: 'slightly', 2735: 'visual', 118: 'appealing', 1912: 'price', 1175: 'heroes', 1928: 'producers', 1007: 'franchise', 2205: 'sequel', 339: 'carry', 2022: 'reference', 68: 'albeit', 2318: 'sorry', 2105: 'robin', 1142: 'harris', 295: 'brothers', 1926: 'produced', 2227: 'shallow', 678: 'disaster', 1769: 'outer', 275: 'brave', 1572: 'max', 2569: 'throw', 2564: 'thousand', 2424: 'students', 1579: 'media', 1201: 'holes', 1885: 'position', 2116: 'room', 847: 'experienced', 2518: 'ted', 1429: 'lawyer', 2457: 'suits', 206: 'beat', 745: 'eat', 2574: 'tight', 2666: 'understanding', 1320: 'irritating', 1773: 'overall', 461: 'comparison', 180: 'awesome', 2446: 'sucks', 419: 'clear', 874: 'factor', 2481: 'surrounding', 1205: 'honest', 2238: 'shocking', 1550: 'manner', 845: 'expensive', 208: 'beautifully', 941: 'filmed', 684: 'disgusting', 483: 'considered', 914: 'feature', 2037: 'released', 3: 'absolute', 2250: 'showing', 2438: 'subtle', 227: 'bet', 2031: 'relate', 389: 'chick', 556: 'crude', 2619: 'tree', 329: 'captured', 165: 'attitude', 1807: 'pathetic', 1143: 'harry', 695: 'doing', 1932: 'professional', 392: 'childhood', 973: 'focus', 748: 'eddie', 2809: 'winning', 2463: 'super', 435: 'college', 615: 'delightful', 747: 'ed', 2044: 'remarkable', 2175: 'scripts', 2095: 'rip', 256: 'bond', 2601: 'tour', 1306: 'international', 2374: 'station', 2859: 'zero', 192: 'bar', 2625: 'tries', 797: 'epic', 405: 'christopher', 1443: 'lee', 330: 'captures', 231: 'biggest', 1636: 'modern', 1147: 'hate', 950: 'finds', 277: 'breaking', 1603: 'mexican', 1329: 'jail', 1813: 'peace', 1747: 'officer', 193: 'barbara', 1946: 'proud', 1417: 'larry', 402: 'chris', 299: 'brutal', 1300: 'intended', 1309: 'interview', 1371: 'keeps', 2290: 'sleazy', 245: 'bloody', 1333: 'japanese', 966: 'flick', 188: 'balance', 539: 'creature', 2135: 'safe', 693: 'dog', 1058: 'girls', 1156: 'heads', 2511: 'team', 1118: 'guys', 1837: 'picked', 1669: 'murderer', 1986: 'random', 424: 'clips', 2851: 'yeah', 1937: 'promise', 1345: 'jobs', 527: 'covered', 2679: 'unknown', 1184: 'highlights', 815: 'event', 506: 'cool', 1775: 'overly', 512: 'correct', 785: 'enjoying', 2570: 'throwing', 625: 'depiction', 449: 'comical', 1377: 'kicks', 1472: 'limited', 2645: 'twice', 949: 'finding', 2084: 'reviews', 553: 'critics', 2664: 'underrated', 2019: 'record', 2617: 'treated', 1310: 'interviews', 1257: 'immediately', 2668: 'unexpected', 980: 'follows', 1655: 'moved', 1590: 'memories', 996: 'forgotten', 1989: 'rare', 1264: 'impression', 1153: 'hbo', 1614: 'millions', 1270: 'incredible', 1045: 'genius', 1311: 'intriguing', 268: 'bound', 2678: 'universe', 1301: 'intense', 1638: 'moment', 2074: 'returns', 682: 'discovers', 2414: 'string', 1893: 'powers', 2310: 'solve', 2104: 'roberts', 9: 'accents', 2628: 'trite', 144: 'ask', 1100: 'gross', 2276: 'sit', 86: 'ambitious', 939: 'filled', 578: 'dangerous', 99: 'animal', 1409: 'lake', 426: 'closely', 170: 'australian', 906: 'fate', 2750: 'walter', 930: 'fiction', 480: 'connection', 2776: 'weak', 2173: 'screenwriter', 646: 'develop', 2412: 'stretch', 1241: 'ideas', 1832: 'phone', 1974: 'quickly', 2422: 'stuck', 1478: 'listening', 1997: 'reach', 2662: 'unconvincing', 954: 'finished', 994: 'forgettable', 1679: 'narration', 240: 'bland', 580: 'danny', 1744: 'offered', 1351: 'joke', 2824: 'wondering', 720: 'drew', 953: 'finish', 210: 'bed', 2599: 'touching', 1574: 'mean', 2647: 'twisted', 1304: 'interested', 1546: 'manage', 2000: 'reading', 2063: 'required', 8: 'accent', 1349: 'johnson', 1568: 'matt', 204: 'beach', 2026: 'regard', 258: 'books', 157: 'attack', 1958: 'pulled', 2445: 'sucked', 1403: 'lacked', 2107: 'rock', 564: 'current', 1920: 'private', 2136: 'said', 2134: 'sadly', 325: 'candy', 1277: 'industry', 2548: 'theaters', 2093: 'ring', 74: 'alike', 1869: 'pointless', 1177: 'hey', 2727: 'villain', 2156: 'scares', 1605: 'michelle', 2734: 'visit', 624: 'depicted', 2816: 'witness', 1062: 'glad', 2573: 'tied', 2709: 'vehicle', 1735: 'obsession', 2102: 'rob', 563: 'curious', 324: 'canadian', 554: 'cross', 2055: 'rented', 451: 'comment', 537: 'creation', 2677: 'universal', 71: 'alice', 1518: 'loves', 2235: 'ship', 2783: 'wedding', 1069: 'gold', 1685: 'natural', 2787: 'weight', 2271: 'sings', 2190: 'seeking', 2858: 'youth', 1812: 'paying', 167: 'attractive', 1442: 'led', 1020: 'friendship', 2421: 'struggling', 976: 'folks', 1343: 'joan', 2050: 'reminiscent', 2550: 'theatrical', 138: 'artistic', 1259: 'importance', 336: 'caring', 1888: 'possibly', 2517: 'technology', 2336: 'spectacular', 2020: 'red', 1500: 'lord', 579: 'daniel', 1459: 'lewis', 1770: 'outrageous', 297: 'brown', 983: 'foot', 1845: 'pity', 386: 'cheese', 1376: 'kick', 101: 'animated', 189: 'ball', 881: 'fairly', 837: 'exist', 1316: 'involving', 2431: 'stupidity', 459: 'compare', 2665: 'understand', 661: 'difference', 1278: 'inept', 394: 'chilling', 1911: 'previously', 1849: 'plain', 1245: 'idiotic', 1420: 'latest', 1370: 'keeping', 1455: 'letter', 1916: 'princess', 349: 'castle', 485: 'consists', 1666: 'multiple', 1504: 'losing', 2051: 'remote', 1818: 'perform', 2343: 'spin', 1089: 'graphics', 1032: 'games', 2464: 'superb', 1292: 'inspired', 1448: 'length', 1529: 'madness', 343: 'cartoons', 2845: 'writers', 79: 'allows', 492: 'content', 1689: 'near', 1583: 'meeting', 2705: 'van', 2663: 'underground', 928: 'festival', 377: 'charlie', 670: 'directly', 1972: 'questions', 145: 'asked', 1692: 'necessarily', 833: 'excuse', 2350: 'spoken', 2372: 'statement', 2549: 'theatre', 211: 'beer', 2229: 'shape', 2289: 'slasher', 448: 'comic', 1276: 'individual', 1460: 'lie', 1618: 'minds', 1297: 'intellectual', 778: 'energy', 1834: 'physical', 1035: 'garbage', 2204: 'separate', 311: 'buy', 2177: 'sean', 1127: 'handle', 312: 'buying', 280: 'breathtaking', 704: 'downright', 2096: 'ripped', 91: 'ancient', 1387: 'kim', 686: 'disney', 2815: 'witch', 2653: 'ugly', 1524: 'ludicrous', 2420: 'struggles', 283: 'briefly', 1877: 'pop', 2716: 'victor', 1684: 'native', 1894: 'practically', 2390: 'stick', 42: 'admittedly', 2613: 'trash', 2700: 'vacation', 2657: 'ultra', 243: 'blonde', 2471: 'supposedly', 1578: 'meant', 626: 'depressing', 134: 'arrives', 515: 'costume', 2425: 'studio', 1842: 'pieces', 1130: 'handsome', 1908: 'pretentious', 2128: 'runs', 2615: 'treasure', 1128: 'handled', 413: 'claim', 108: 'answers', 2620: 'trek', 546: 'crew', 1970: 'quest', 1212: 'hopes', 364: 'chair', 1113: 'guilty', 1315: 'involves', 2119: 'round', 414: 'claims', 1585: 'melodrama', 931: 'fictional', 959: 'flashback', 408: 'cinematic', 1542: 'makeup', 1914: 'prime', 1192: 'history', 2292: 'sleeping', 1331: 'jane', 666: 'direct', 509: 'copy', 2849: 'wrote', 399: 'choose', 2041: 'remain', 1762: 'opposite', 2397: 'stopped', 2056: 'renting', 2081: 'review', 2299: 'smile', 363: 'cgi', 2100: 'river', 2004: 'realistic', 2486: 'suspenseful', 1200: 'hole', 1772: 'outstanding', 775: 'endless', 381: 'chases', 380: 'chase', 713: 'draw', 699: 'donald', 225: 'ben', 1021: 'frightening', 1302: 'intensity', 1488: 'locations', 1509: 'loud', 2059: 'repeatedly', 2160: 'scenery', 1180: 'hiding', 2039: 'religion', 1792: 'paris', 397: 'choice', 171: 'authentic', 1510: 'louis', 2182: 'seat', 1761: 'opposed', 1764: 'ordinary', 1082: 'government', 494: 'continue', 2124: 'rule', 917: 'featuring', 2539: 'test', 271: 'boyfriend', 385: 'checking', 1663: 'ms', 2715: 'victims', 526: 'cover', 1119: 'hadn', 984: 'footage', 434: 'collection', 2381: 'steals', 222: 'believes', 2038: 'relief', 2520: 'teen', 1437: 'learning', 254: 'body', 249: 'blue', 2389: 'stewart', 2466: 'supernatural', 887: 'falling', 1364: 'jungle', 186: 'badly', 2588: 'tongue', 1405: 'lacks', 2199: 'send', 350: 'cat', 2282: 'size', 1313: 'introduction', 595: 'deals', 2861: 'zombies', 1597: 'mere', 2687: 'ups', 524: 'court', 2674: 'uninteresting', 2747: 'walking', 2200: 'sends', 45: 'advantage', 2805: 'win', 241: 'blind', 155: 'atmospheric', 2163: 'sci', 929: 'fi', 1586: 'melodramatic', 1244: 'idiot', 2792: 'west', 2576: 'tim', 2222: 'sexual', 2684: 'unrealistic', 623: 'department', 1606: 'mid', 1778: 'paced', 2197: 'selling', 805: 'escape', 1833: 'photography', 2757: 'warm', 2589: 'tony', 633: 'deserves', 593: 'deal', 464: 'competition', 601: 'debut', 318: 'calls', 2101: 'road', 922: 'feet', 2509: 'teach', 307: 'bus', 2112: 'rolling', 1190: 'hired', 1492: 'lonely', 650: 'device', 2154: 'scare', 1424: 'laughing', 152: 'assistant', 723: 'drive', 727: 'driving', 641: 'destroyed', 1552: 'market', 1955: 'psychological', 1511: 'lousy', 763: 'embarrassed', 2300: 'smith', 1783: 'paid', 164: 'attention', 543: 'credit', 2604: 'track', 182: 'awkward', 439: 'com', 2498: 'talents', 1854: 'plastic', 342: 'cartoon', 128: 'appropriate', 525: 'cousin', 860: 'extra', 916: 'features', 491: 'contemporary', 1717: 'notable', 1487: 'location', 1941: 'proper', 992: 'forever', 799: 'episodes', 1041: 'gene', 218: 'belief', 2746: 'walked', 279: 'breath', 2232: 'sheer', 1729: 'numbers', 1146: 'hat', 93: 'andy', 126: 'appreciated', 313: 'cable', 1096: 'green', 142: 'asian', 856: 'express', 721: 'drink', 2388: 'steven', 2478: 'surprising', 2082: 'reviewer', 1354: 'joseph', 2503: 'talks', 2341: 'spends', 70: 'alex', 834: 'executed', 382: 'chasing', 1969: 'queen', 443: 'comedian', 1811: 'pay', 962: 'flaw', 988: 'forces', 2255: 'sides', 1191: 'historical', 1570: 'matters', 1043: 'generally', 1987: 'range', 105: 'annoyed', 40: 'adds', 2706: 'variety', 616: 'deliver', 852: 'explained', 370: 'changing', 981: 'food', 722: 'drinking', 1249: 'ill', 10: 'accept', 1090: 'gratuitous', 2384: 'steps', 2304: 'society', 2333: 'speaks', 2544: 'thankfully', 577: 'danger', 2786: 'weeks', 2324: 'sounds', 1643: 'month', 2377: 'stayed', 2007: 'realized', 989: 'ford', 968: 'flight', 2015: 'recently', 468: 'complicated', 418: 'clean', 2264: 'simon', 1319: 'irony', 1404: 'lacking', 2272: 'sinister', 868: 'fabulous', 1374: 'kevin', 2359: 'standard', 2346: 'split', 1055: 'giant', 2335: 'specific', 2597: 'touched', 636: 'desire', 131: 'arm', 2218: 'setting', 57: 'ages', 34: 'adaptation', 351: 'catch', 2305: 'soft', 1850: 'plan', 551: 'critical', 457: 'community', 1048: 'genuinely', 2143: 'satisfying', 1904: 'presentation', 1000: 'forth', 2513: 'technical', 2437: 'subtitles', 572: 'daily', 375: 'charisma', 2393: 'stomach', 1560: 'mask', 2169: 'screaming', 1037: 'gas', 761: 'elizabeth', 932: 'field', 1616: 'minded', 1346: 'joe', 1982: 'rain', 1804: 'passion', 1247: 'ignored', 1137: 'happily', 2382: 'step', 2608: 'tragic', 870: 'faced', 194: 'bare', 1126: 'handed', 1571: 'mature', 1155: 'headed', 2060: 'replaced', 2168: 'scream', 2795: 'whatsoever', 811: 'established', 2034: 'relationships', 2118: 'rough', 1018: 'friendly', 1892: 'powerful', 2030: 'regular', 1093: 'greater', 511: 'corny', 1198: 'holding', 310: 'butt', 80: 'alright', 2011: 'reasons', 493: 'context', 1470: 'likely', 1800: 'party', 2032: 'related', 2311: 'somebody', 1407: 'lady', 1530: 'magic', 1307: 'internet', 1112: 'guide', 293: 'broken', 2353: 'spot', 139: 'artists', 1793: 'park', 1867: 'poignant', 1782: 'page', 1623: 'mirror', 1030: 'gags', 1918: 'prior', 548: 'criminal', 276: 'break', 944: 'filmmakers', 107: 'answer', 1881: 'portrayal', 2434: 'sub', 960: 'flashbacks', 765: 'emotion', 905: 'fat', 728: 'drop', 734: 'dude', 2813: 'wishes', 1138: 'happiness', 1286: 'innocence', 1086: 'grant', 1298: 'intelligence', 44: 'adults', 955: 'fish', 2688: 'upset', 619: 'delivery', 2215: 'service', 810: 'essentially', 298: 'bruce', 1016: 'friday', 717: 'dreams', 1334: 'jason', 839: 'existent', 2423: 'student', 1563: 'massive', 2427: 'study', 2553: 'theory', 2369: 'starting', 1290: 'insight', 998: 'format', 246: 'blow', 2164: 'science', 427: 'closer', 281: 'brian', 1631: 'mistakes', 819: 'everyday', 1131: 'hang', 166: 'attraction', 2178: 'search', 1282: 'information', 2540: 'texas', 201: 'batman', 2760: 'warner', 2375: 'status', 198: 'basic', 2181: 'seasons', 47: 'adventures', 1703: 'nicely', 2774: 'wayne', 252: 'bob', 1003: 'fourth', 410: 'circumstances', 1001: 'fortunately', 2241: 'shoots', 300: 'bucks', 2296: 'slowly', 1195: 'hits', 1826: 'personalities', 247: 'blown', 800: 'equal', 2477: 'surprises', 795: 'entry', 1187: 'hill', 2686: 'upper', 2418: 'structure', 81: 'amateur', 1164: 'heavily', 1252: 'images', 1944: 'protagonists', 1099: 'gritty', 1934: 'profound', 1938: 'promising', 866: 'eyed', 2014: 'recent', 1980: 'racist', 261: 'boredom', 2669: 'unforgettable', 2321: 'soul', 1080: 'gotta', 879: 'failure', 502: 'convey', 1873: 'politics', 861: 'extraordinary', 1308: 'interpretation', 288: 'bringing', 1627: 'missed', 832: 'exciting', 2655: 'ultimate', 738: 'dying', 2658: 'unable', 1254: 'imaginative', 41: 'admit', 504: 'convinced', 2052: 'remotely', 1743: 'offer', 1123: 'hall', 104: 'anne', 2703: 'vampire', 1724: 'novels', 1188: 'hint', 965: 'flesh', 974: 'focused', 1581: 'mediocre', 255: 'bomb', 438: 'colors', 2484: 'suspect', 1400: 'kudos', 2676: 'united', 159: 'attacks', 557: 'cruel', 1288: 'insane', 1229: 'humanity', 1012: 'freedom', 2337: 'speech', 1489: 'locked', 6: 'abuse', 1642: 'monsters', 398: 'choices', 2453: 'suggest', 1809: 'patrick', 2660: 'uncle', 1760: 'opportunity', 1251: 'imagery', 583: 'darkness', 2196: 'sell', 221: 'believed', 718: 'dress', 432: 'code', 555: 'crowd', 1178: 'hidden', 1951: 'provided', 2800: 'wild', 1335: 'jean', 725: 'driver', 2131: 'russian', 1482: 'lived', 1004: 'fox', 995: 'forgot', 1450: 'lesser', 1447: 'legs', 2449: 'suffer', 2090: 'riding', 1611: 'military', 1081: 'gotten', 2048: 'reminded', 642: 'destruction', 1390: 'kinds', 2723: 'viewers', 472: 'concerns', 2850: 'wwii', 598: 'dear', 909: 'favor', 777: 'enemy', 1009: 'frankly', 1898: 'prefer', 803: 'eric', 2583: 'titles', 2234: 'shines', 1896: 'pre', 1356: 'joy', 2130: 'russell', 1844: 'pilot', 2531: 'tense', 538: 'creative', 2685: 'unusual', 1430: 'lazy', 649: 'develops', 1540: 'makers', 963: 'flawed', 371: 'channel', 436: 'color', 2575: 'till', 2155: 'scared', 2456: 'suit', 1471: 'likes', 2242: 'shop', 2416: 'strongly', 685: 'dislike', 77: 'allow', 857: 'expression', 429: 'clothes', 2530: 'tend', 1561: 'mass', 694: 'dogs', 514: 'costs', 1077: 'gorgeous', 224: 'beloved', 608: 'decision', 2293: 'slight', 560: 'cultural', 48: 'advice', 2458: 'sum', 2120: 'routine', 1601: 'met', 1539: 'maker', 187: 'bag', 2352: 'sports', 2106: 'robot', 1226: 'howard', 2808: 'winner', 2510: 'teacher', 1232: 'humorous', 2286: 'skip', 1644: 'months', 205: 'bear', 2806: 'wind', 1415: 'largely', 2603: 'toy', 341: 'cars', 1451: 'lesson', 358: 'center', 103: 'ann', 1668: 'murdered', 2189: 'seek', 172: 'author', 2355: 'spy', 83: 'amazed', 2525: 'teeth', 1995: 'raw', 147: 'asks', 2115: 'ron', 1795: 'parody', 1044: 'generation', 85: 'amazingly', 1964: 'push', 2712: 'veteran', 2351: 'spoof', 1802: 'passed', 743: 'east', 2123: 'ruined', 1798: 'partner', 1662: 'mrs', 2287: 'sky', 1806: 'path', 1851: 'plane', 1706: 'nightmare', 2385: 'stereotypes', 1742: 'offensive', 156: 'atrocious', 1682: 'nation', 1272: 'independent', 1862: 'pleasure', 915: 'featured', 1280: 'infamous', 1239: 'ice', 711: 'dramas', 2433: 'stylish', 374: 'charge', 1216: 'horrific', 495: 'continues', 1981: 'radio', 455: 'committed', 441: 'combined', 2842: 'wow', 239: 'blame', 2401: 'storm', 478: 'confusion', 2461: 'sun', 531: 'crash', 971: 'fly', 1774: 'overcome', 808: 'essence', 638: 'desperately', 232: 'billy', 2614: 'travel', 829: 'exceptional', 2194: 'segment', 1294: 'instantly', 2347: 'spoil', 620: 'demon', 0: 'abandoned', 621: 'demons', 2028: 'regardless', 463: 'competent', 1087: 'granted', 597: 'dean', 552: 'criticism', 982: 'fool', 2506: 'task', 1766: 'originality', 67: 'alas', 990: 'foreign', 1978: 'quote', 2027: 'regarding', 2639: 'turkey', 911: 'favorites', 890: 'fame', 2245: 'shorts', 2256: 'sight', 2220: 'seven', 1646: 'moon', 1525: 'lying', 484: 'considering', 2761: 'warning', 2356: 'st', 148: 'asleep', 570: 'cynical', 558: 'crying', 428: 'closing', 2626: 'trilogy', 712: 'dramatic', 2450: 'suffered', 1029: 'gag', 889: 'false', 1526: 'lynch', 2672: 'unfunny', 355: 'caused', 1299: 'intelligent', 2225: 'shadow', 52: 'african', 404: 'christmas', 1283: 'initial', 1598: 'merely', 1427: 'laura', 1106: 'grows', 1704: 'nick', 2758: 'warn', 2572: 'throws', 2621: 'tribute', 1210: 'hoped', 2167: 'scott', 786: 'enjoyment', 53: 'afternoon', 1656: 'movement', 291: 'broadway', 1203: 'homage', 2097: 'rise', 2749: 'wall', 1074: 'goofy', 1014: 'frequently', 2008: 'realizes', 2631: 'truck', 2404: 'storytelling', 2069: 'result', 1005: 'frame', 2440: 'succeeds', 755: 'effectively', 2108: 'roger', 2062: 'reputation', 2340: 'spending', 1602: 'metal', 1895: 'praise', 1350: 'join', 767: 'emotionally', 2748: 'walks', 396: 'chinese', 73: 'aliens', 2237: 'shocked', 66: 'alan', 632: 'deserved', 1948: 'proved', 674: 'disagree', 2139: 'san', 2732: 'virtually', 542: 'credible', 2650: 'types', 1064: 'glimpse', 2810: 'wins', 1953: 'provoking', 1499: 'loose', 1234: 'hundreds', 1215: 'horribly', 440: 'combination', 1665: 'multi', 2202: 'sensitive', 1416: 'larger', 892: 'families', 2711: 'versions', 859: 'extent', 2212: 'serve', 1822: 'performers', 1359: 'julia', 1580: 'medical', 1696: 'needless', 379: 'charming', 2054: 'rental', 1332: 'japan', 1732: 'object', 340: 'carrying', 1348: 'johnny', 655: 'dick', 352: 'category', 176: 'award', 33: 'adam', 1360: 'julie', 764: 'embarrassing', 2267: 'sing', 991: 'forest', 1051: 'germany', 2330: 'spanish', 337: 'carried', 2729: 'vincent', 161: 'attempted', 1861: 'pleasant', 836: 'exercise', 772: 'endearing', 1734: 'obsessed', 1466: 'lights', 1739: 'occasionally', 2698: 'utter', 2439: 'succeed', 696: 'dollar', 1576: 'meaningful', 1716: 'nose', 1803: 'passing', 1725: 'nowadays', 2767: 'wasting', 708: 'dragged', 2275: 'sisters', 1847: 'placed', 1408: 'laid', 1279: 'inevitable', 2579: 'timing', 920: 'feelings', 1874: 'pool', 2364: 'starred', 2147: 'saves', 806: 'escapes', 2744: 'wake', 2667: 'understood', 2443: 'successfully', 603: 'decades', 1293: 'instance', 1338: 'jerry', 2398: 'stops', 687: 'display', 938: 'figures', 500: 'controversial', 266: 'bothered', 2704: 'vampires', 565: 'curse', 1392: 'kiss', 390: 'chief', 121: 'appearances', 233: 'birth', 1931: 'productions', 503: 'convince', 200: 'basis', 787: 'ensemble', 2560: 'thomas', 1236: 'hunter', 1189: 'hip', 334: 'carefully', 528: 'crafted', 635: 'designed', 403: 'christian', 2079: 'revelation', 1303: 'intentions', 958: 'flash', 622: 'dennis', 1323: 'issue', 1907: 'president', 2689: 'urban', 1025: 'funnier', 1375: 'key', 1942: 'properly', 1838: 'picks', 1336: 'jeff', 305: 'built', 1445: 'legend', 1915: 'prince', 2179: 'searching', 2623: 'tricks', 2233: 'sheriff', 1612: 'miller', 2673: 'uninspired', 561: 'culture', 665: 'dinner', 92: 'anderson', 11: 'acceptable', 1182: 'higher', 569: 'cutting', 94: 'angel', 1998: 'reaction', 2380: 'stealing', 2277: 'sitcom', 2345: 'spite', 2326: 'source', 141: 'ashamed', 2499: 'tales', 629: 'description', 2413: 'striking', 2508: 'taylor', 323: 'campy', 1776: 'owner', 2693: 'useless', 2652: 'typically', 1285: 'inner', 2223: 'sexuality', 2719: 'vietnam'}\n",
      "(2862,)\n"
     ]
    }
   ],
   "source": [
    "data = dataset(path_of_data)\n",
    "train_loader = DataLoader(data, batch_size=4096)\n",
    "print(data[5][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model definition\n",
    "class classifier(nn.Module):\n",
    "    def __init__(self,vocab_size,hidden1,hidden2):\n",
    "        super(classifier,self).__init__()\n",
    "        self.fc1=nn.Linear(vocab_size,hidden1)\n",
    "        self.fc2=nn.Linear(hidden1,hidden2)\n",
    "        self.fc3=nn.Linear(hidden2,1)\n",
    "    def forward(self,inputs):\n",
    "        x=F.relu(self.fc1(inputs.squeeze(1).float()))\n",
    "        print(x)\n",
    "        x=F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier(\n",
       "  (fc1): Linear(in_features=2862, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=classifier(len(data.token2idx), 128, 64)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.BCEWithLogitsLoss()\n",
    "#print([p for p in model.parameters()])\n",
    "optim=optim.Adam([p for p in model.parameters() if p.requires_grad], lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91884\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0732, 0.0000, 0.0000,  ..., 0.1030, 0.0051, 0.0464],\n",
      "        [0.0535, 0.0000, 0.0000,  ..., 0.1515, 0.0868, 0.1060],\n",
      "        [0.1189, 0.0000, 0.0000,  ..., 0.0718, 0.1730, 0.0833],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.1704,  ..., 0.0879, 0.0000, 0.0000],\n",
      "        [0.0936, 0.1630, 0.0000,  ..., 0.0000, 0.0552, 0.0000],\n",
      "        [0.2023, 0.1576, 0.0000,  ..., 0.1520, 0.0863, 0.0895]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.0866, 0.0000,  ..., 0.0607, 0.0360, 0.0295],\n",
      "        [0.1333, 0.0000, 0.2188,  ..., 0.0232, 0.0572, 0.0754],\n",
      "        [0.0000, 0.0068, 0.0000,  ..., 0.0636, 0.0000, 0.0038],\n",
      "        ...,\n",
      "        [0.2235, 0.1883, 0.0000,  ..., 0.0000, 0.0391, 0.1279],\n",
      "        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0108, 0.0000],\n",
      "        [0.0070, 0.0842, 0.0677,  ..., 0.0000, 0.0581, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.1142, 0.0346, 0.0000,  ..., 0.2654, 0.0651, 0.1424],\n",
      "        [0.1857, 0.4368, 0.0000,  ..., 0.0000, 0.2129, 0.0836],\n",
      "        [0.1225, 0.0032, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0884, 0.0160,  ..., 0.1038, 0.0000, 0.0000],\n",
      "        [0.2400, 0.0000, 0.0000,  ..., 0.0000, 0.0125, 0.0000],\n",
      "        [0.0936, 0.1822, 0.0000,  ..., 0.0000, 0.0000, 0.0025]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.2069, 0.0035, 0.0076,  ..., 0.0000, 0.0000, 0.0130],\n",
      "        [0.0718, 0.0232, 0.0500,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1510, 0.0000,  ..., 0.0000, 0.0000, 0.1544],\n",
      "        ...,\n",
      "        [0.0553, 0.0000, 0.0316,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0560, 0.0000,  ..., 0.0000, 0.0000, 0.0860],\n",
      "        [0.0000, 0.0000, 0.1176,  ..., 0.0571, 0.0000, 0.0720]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0293, 0.0000, 0.0000,  ..., 0.1225, 0.0000, 0.0682],\n",
      "        [0.1231, 0.0457, 0.0135,  ..., 0.0042, 0.1108, 0.0961],\n",
      "        [0.2483, 0.0000, 0.0388,  ..., 0.0000, 0.0000, 0.0029],\n",
      "        ...,\n",
      "        [0.0000, 0.0162, 0.0938,  ..., 0.0944, 0.0153, 0.0000],\n",
      "        [0.0000, 0.2224, 0.0944,  ..., 0.0890, 0.0413, 0.0000],\n",
      "        [0.1483, 0.1316, 0.0964,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.2364, 0.0740, 0.0716,  ..., 0.0632, 0.0000, 0.0107],\n",
      "        [0.0000, 0.3074, 0.0785,  ..., 0.0513, 0.0000, 0.0000],\n",
      "        [0.7198, 0.8683, 0.3619,  ..., 0.4006, 0.0000, 0.5187],\n",
      "        ...,\n",
      "        [0.1785, 0.2614, 0.0457,  ..., 0.0000, 0.0000, 0.0616],\n",
      "        [0.1916, 0.1357, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3418, 0.3436,  ..., 0.0708, 0.0000, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.2037, 0.3872, 0.1627,  ..., 0.0611, 0.0000, 0.1611],\n",
      "        [0.1654, 0.0812, 0.0948,  ..., 0.1575, 0.0000, 0.2740],\n",
      "        [1.0606, 0.5455, 0.2899,  ..., 0.0000, 0.0000, 0.9448],\n",
      "        ...,\n",
      "        [0.2463, 0.0000, 0.0179,  ..., 0.0000, 0.0000, 0.2893],\n",
      "        [0.1722, 0.1526, 0.1170,  ..., 0.1299, 0.0000, 0.1627],\n",
      "        [0.0673, 0.1916, 0.2424,  ..., 0.0000, 0.0000, 0.0251]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.1615, 0.3657, 0.0597,  ..., 0.2548, 0.0000, 0.1202],\n",
      "        [0.0033, 0.1571, 0.0623,  ..., 0.0514, 0.0000, 0.0551],\n",
      "        [0.0000, 0.2412, 0.1066,  ..., 0.0000, 0.0000, 0.1150],\n",
      "        ...,\n",
      "        [0.1220, 0.2951, 0.0000,  ..., 0.0000, 0.0000, 0.4673],\n",
      "        [0.5977, 0.3744, 0.3264,  ..., 0.0000, 0.0000, 0.4631],\n",
      "        [0.0465, 0.1059, 0.1815,  ..., 0.0000, 0.0000, 0.1091]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000, 0.2280, 0.1800,  ..., 0.0000, 0.0000, 0.0153],\n",
      "        [0.1688, 0.0653, 0.1017,  ..., 0.0000, 0.0000, 0.2340],\n",
      "        [0.0000, 0.3639, 0.2094,  ..., 0.1066, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.4693, 0.0127,  ..., 0.0397, 0.0000, 0.0231],\n",
      "        [0.0380, 0.3948, 0.2917,  ..., 0.0000, 0.0000, 0.0236],\n",
      "        [0.4048, 0.1728, 0.2371,  ..., 0.0058, 0.0000, 0.2021]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.1576, 0.3073, 0.3409,  ..., 0.1622, 0.0000, 0.0000],\n",
      "        [0.1325, 0.1881, 0.2949,  ..., 0.0000, 0.0000, 0.1176],\n",
      "        [0.1458, 0.0058, 0.1165,  ..., 0.0000, 0.0000, 0.0346],\n",
      "        ...,\n",
      "        [0.0556, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3018],\n",
      "        [0.0269, 0.2605, 0.3080,  ..., 0.0276, 0.0000, 0.0000],\n",
      "        [0.4209, 0.4234, 0.1103,  ..., 0.0190, 0.0240, 0.2044]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.4832, 0.4074, 0.3019,  ..., 0.2503, 0.0000, 0.5197],\n",
      "        [0.2122, 0.4088, 0.3516,  ..., 0.2839, 0.0000, 0.0484],\n",
      "        [0.2181, 0.0686, 0.0000,  ..., 0.0417, 0.0000, 0.2037],\n",
      "        ...,\n",
      "        [0.1039, 0.0752, 0.0674,  ..., 0.0000, 0.0000, 0.0533],\n",
      "        [0.2206, 0.0279, 0.0674,  ..., 0.0000, 0.0000, 0.2163],\n",
      "        [0.4446, 0.1649, 0.0176,  ..., 0.0000, 0.0000, 0.1022]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3277, 0.2386, 0.2409,  ..., 0.2388, 0.0000, 0.2123],\n",
      "        [0.1455, 0.0874, 0.1016,  ..., 0.0057, 0.0000, 0.0554],\n",
      "        [0.1318, 0.4967, 0.3245,  ..., 0.1403, 0.0000, 0.0282],\n",
      "        ...,\n",
      "        [0.0694, 0.0000, 0.0507,  ..., 0.0000, 0.0000, 0.0681],\n",
      "        [0.4126, 0.0920, 0.0000,  ..., 0.0000, 0.0000, 0.3617],\n",
      "        [0.6858, 0.2159, 0.0000,  ..., 0.0340, 0.0000, 0.6708]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.1437, 0.2013, 0.1273,  ..., 0.0456, 0.0000, 0.0157],\n",
      "        [0.8520, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.7224],\n",
      "        [0.7329, 0.3501, 0.2879,  ..., 0.1534, 0.0000, 0.9848],\n",
      "        ...,\n",
      "        [0.4602, 0.3808, 0.1472,  ..., 0.1685, 0.0000, 0.3885],\n",
      "        [0.1591, 0.1331, 0.0217,  ..., 0.0410, 0.0000, 0.3046],\n",
      "        [0.2549, 0.4381, 0.1903,  ..., 0.0000, 0.0000, 0.2526]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Epoch #1\tTrain Loss: 0.630\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6420, 0.3671, 0.1619,  ..., 0.1540, 0.0000, 0.6141],\n",
      "        [0.4865, 0.1879, 0.0519,  ..., 0.0721, 0.0000, 0.5880],\n",
      "        [0.4854, 0.2199, 0.0779,  ..., 0.0465, 0.0000, 0.4844],\n",
      "        ...,\n",
      "        [0.4354, 0.0000, 0.1207,  ..., 0.0000, 0.0000, 0.2779],\n",
      "        [0.4437, 0.3800, 0.0000,  ..., 0.0000, 0.0000, 0.2339],\n",
      "        [0.7833, 0.7029, 0.3112,  ..., 0.2407, 0.0000, 0.6725]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0761, 0.5787, 0.3284,  ..., 0.3847, 0.0000, 0.1265],\n",
      "        [0.6268, 0.5010, 0.6058,  ..., 0.2707, 0.0000, 0.6179],\n",
      "        [0.2772, 0.5481, 0.3047,  ..., 0.3081, 0.0000, 0.3027],\n",
      "        ...,\n",
      "        [1.7505, 0.7183, 0.1677,  ..., 0.0000, 0.0000, 1.7284],\n",
      "        [0.9059, 0.1228, 0.0000,  ..., 0.0000, 0.0000, 0.7720],\n",
      "        [0.2332, 0.1473, 0.0870,  ..., 0.0000, 0.0000, 0.1606]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.6273, 0.3381, 0.0814,  ..., 0.3432, 0.0000, 0.6684],\n",
      "        [1.1635, 1.5302, 0.7455,  ..., 0.3377, 0.0000, 1.1503],\n",
      "        [0.6530, 0.2514, 0.1269,  ..., 0.0000, 0.0000, 0.5402],\n",
      "        ...,\n",
      "        [0.1974, 0.3950, 0.2207,  ..., 0.2836, 0.0000, 0.1606],\n",
      "        [0.5783, 0.3292, 0.2010,  ..., 0.1040, 0.0000, 0.3545],\n",
      "        [0.5920, 0.1536, 0.0000,  ..., 0.0000, 0.0000, 0.4979]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.6818, 0.1499, 0.0447,  ..., 0.0000, 0.0000, 0.5083],\n",
      "        [0.2678, 0.0519, 0.0433,  ..., 0.0000, 0.0000, 0.1663],\n",
      "        [0.7181, 0.5838, 0.1602,  ..., 0.0990, 0.0000, 0.9689],\n",
      "        ...,\n",
      "        [0.3858, 0.4615, 0.4973,  ..., 0.3090, 0.0000, 0.3229],\n",
      "        [0.6386, 0.6863, 0.2647,  ..., 0.1615, 0.0000, 0.9308],\n",
      "        [0.0954, 0.2327, 0.3431,  ..., 0.2659, 0.0000, 0.1798]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3405, 0.3269, 0.2417,  ..., 0.3764, 0.0000, 0.4070],\n",
      "        [0.4365, 0.2845, 0.1361,  ..., 0.1595, 0.0000, 0.4307],\n",
      "        [0.4732, 0.1473, 0.1399,  ..., 0.0361, 0.0000, 0.2775],\n",
      "        ...,\n",
      "        [0.2263, 0.2609, 0.2200,  ..., 0.2171, 0.0000, 0.1811],\n",
      "        [0.2365, 0.6703, 0.4058,  ..., 0.4407, 0.0000, 0.2752],\n",
      "        [0.7077, 0.4473, 0.1693,  ..., 0.0000, 0.0000, 0.5342]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.6780, 0.1473, 0.0283,  ..., 0.1026, 0.0000, 0.4734],\n",
      "        [0.1918, 0.8641, 0.4562,  ..., 0.4835, 0.0000, 0.0909],\n",
      "        [2.9248, 2.8828, 1.5879,  ..., 1.9149, 0.0000, 3.0194],\n",
      "        ...,\n",
      "        [0.6333, 0.5019, 0.1291,  ..., 0.0976, 0.0000, 0.5432],\n",
      "        [0.4610, 0.4393, 0.1454,  ..., 0.1293, 0.0000, 0.2986],\n",
      "        [0.0135, 0.8813, 0.6971,  ..., 0.5271, 0.0000, 0.1852]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.8777, 1.2139, 0.6826,  ..., 0.7426, 0.0000, 0.8628],\n",
      "        [0.6211, 0.2021, 0.0590,  ..., 0.2379, 0.0000, 0.7663],\n",
      "        [3.5673, 2.1914, 1.1420,  ..., 0.9726, 0.0000, 3.6691],\n",
      "        ...,\n",
      "        [0.7432, 0.0000, 0.0219,  ..., 0.0762, 0.0000, 0.8031],\n",
      "        [0.6806, 0.7056, 0.4462,  ..., 0.6033, 0.0000, 0.7320],\n",
      "        [0.3398, 0.5574, 0.4712,  ..., 0.3158, 0.0000, 0.3466]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.6843, 0.8314, 0.3883,  ..., 0.6685, 0.0000, 0.6892],\n",
      "        [0.4284, 0.4257, 0.1855,  ..., 0.2660, 0.0000, 0.4996],\n",
      "        [0.3996, 0.4293, 0.1161,  ..., 0.0000, 0.0000, 0.5400],\n",
      "        ...,\n",
      "        [0.7852, 0.7482, 0.1946,  ..., 0.3396, 0.0000, 1.1839],\n",
      "        [1.3066, 1.3538, 0.9499,  ..., 0.6834, 0.0000, 1.3140],\n",
      "        [0.4264, 0.4888, 0.3876,  ..., 0.2818, 0.0000, 0.5127]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3786, 0.5169, 0.3133,  ..., 0.2378, 0.0000, 0.4951],\n",
      "        [0.3776, 0.2646, 0.2189,  ..., 0.1431, 0.0000, 0.4678],\n",
      "        [0.1876, 0.7412, 0.4736,  ..., 0.4543, 0.0000, 0.1494],\n",
      "        ...,\n",
      "        [0.2169, 0.8412, 0.3157,  ..., 0.3936, 0.0000, 0.3369],\n",
      "        [0.3943, 0.7854, 0.4951,  ..., 0.2520, 0.0000, 0.4086],\n",
      "        [1.0139, 0.4763, 0.3110,  ..., 0.3291, 0.0000, 0.8508]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.4438, 0.6277, 0.5593,  ..., 0.4565, 0.0000, 0.2909],\n",
      "        [0.4981, 0.4028, 0.3457,  ..., 0.0672, 0.0000, 0.5020],\n",
      "        [0.5068, 0.2726, 0.2472,  ..., 0.2177, 0.0000, 0.4248],\n",
      "        ...,\n",
      "        [0.4292, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.7013],\n",
      "        [0.2192, 0.5199, 0.4749,  ..., 0.2907, 0.0000, 0.2003],\n",
      "        [1.1870, 0.8503, 0.3459,  ..., 0.4475, 0.0000, 1.0021]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1229, 1.2108, 0.9021,  ..., 0.9992, 0.0000, 1.2497],\n",
      "        [0.5978, 0.7302, 0.5326,  ..., 0.5780, 0.0000, 0.4702],\n",
      "        [0.4170, 0.2749, 0.0000,  ..., 0.2176, 0.0000, 0.4269],\n",
      "        ...,\n",
      "        [0.2887, 0.1469, 0.0917,  ..., 0.0507, 0.0000, 0.2483],\n",
      "        [0.5572, 0.1148, 0.1094,  ..., 0.0306, 0.0000, 0.5632],\n",
      "        [0.8388, 0.3310, 0.0809,  ..., 0.1567, 0.0000, 0.5237]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.7897, 0.5659, 0.4818,  ..., 0.5287, 0.0000, 0.7030],\n",
      "        [0.4583, 0.1112, 0.0626,  ..., 0.0362, 0.0000, 0.3780],\n",
      "        [0.4112, 0.8466, 0.5926,  ..., 0.4857, 0.0000, 0.3635],\n",
      "        ...,\n",
      "        [0.2423, 0.0000, 0.0339,  ..., 0.0361, 0.0000, 0.2484],\n",
      "        [0.9193, 0.2186, 0.0000,  ..., 0.1572, 0.0000, 0.9127],\n",
      "        [1.3692, 0.3990, 0.0953,  ..., 0.2151, 0.0000, 1.3762]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.2773, 0.3645, 0.2762,  ..., 0.1931, 0.0000, 0.1618],\n",
      "        [1.5765, 0.1585, 0.0000,  ..., 0.0000, 0.0000, 1.4904],\n",
      "        [1.5080, 0.6237, 0.3833,  ..., 0.3430, 0.0000, 1.7869],\n",
      "        ...,\n",
      "        [0.8634, 0.5989, 0.3369,  ..., 0.4017, 0.0000, 0.8218],\n",
      "        [0.4315, 0.3454, 0.1981,  ..., 0.2484, 0.0000, 0.6083],\n",
      "        [0.5996, 0.6208, 0.2860,  ..., 0.1698, 0.0000, 0.6057]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Epoch #2\tTrain Loss: 0.402\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2897, 0.6369, 0.3676,  ..., 0.3919, 0.0000, 1.3101],\n",
      "        [0.8826, 0.3837, 0.2208,  ..., 0.2585, 0.0000, 1.0248],\n",
      "        [0.8422, 0.3693, 0.1913,  ..., 0.1971, 0.0000, 0.8664],\n",
      "        ...,\n",
      "        [0.8717, 0.0000, 0.0186,  ..., 0.0000, 0.0000, 0.7171],\n",
      "        [0.8476, 0.4451, 0.0000,  ..., 0.0000, 0.0000, 0.6498],\n",
      "        [1.4790, 1.0152, 0.5549,  ..., 0.5345, 0.0000, 1.4016]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[2.6175e-01, 8.7724e-01, 5.9003e-01,  ..., 6.6262e-01, 0.0000e+00,\n",
      "         3.3775e-01],\n",
      "        [1.2458e+00, 6.7845e-01, 6.8129e-01,  ..., 4.3726e-01, 0.0000e+00,\n",
      "         1.2678e+00],\n",
      "        [5.3920e-01, 9.1366e-01, 6.5745e-01,  ..., 6.5855e-01, 0.0000e+00,\n",
      "         6.0083e-01],\n",
      "        ...,\n",
      "        [2.8511e+00, 1.2484e+00, 6.0580e-01,  ..., 4.1669e-01, 0.0000e+00,\n",
      "         2.9174e+00],\n",
      "        [1.6157e+00, 2.3157e-01, 9.4546e-02,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         1.5159e+00],\n",
      "        [4.3003e-01, 1.7796e-01, 1.0942e-01,  ..., 1.3495e-03, 0.0000e+00,\n",
      "         3.6694e-01]], grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0289, 0.4979, 0.2357,  ..., 0.4587, 0.0000, 1.0884],\n",
      "        [2.0538, 2.0819, 1.2177,  ..., 0.8894, 0.0000, 2.1126],\n",
      "        [0.9340, 0.5380, 0.4188,  ..., 0.2681, 0.0000, 0.8636],\n",
      "        ...,\n",
      "        [0.4329, 0.5493, 0.3391,  ..., 0.4426, 0.0000, 0.4081],\n",
      "        [0.8344, 0.5386, 0.3920,  ..., 0.3043, 0.0000, 0.6481],\n",
      "        [0.9181, 0.1553, 0.0000,  ..., 0.0000, 0.0000, 0.8407]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0146, 0.2043, 0.1261,  ..., 0.0505, 0.0000, 0.8587],\n",
      "        [0.4319, 0.0481, 0.0322,  ..., 0.0000, 0.0000, 0.3376],\n",
      "        [1.1665, 0.9181, 0.4907,  ..., 0.4481, 0.0000, 1.4554],\n",
      "        ...,\n",
      "        [0.7513, 0.7210, 0.7305,  ..., 0.5262, 0.0000, 0.7034],\n",
      "        [1.1507, 1.0307, 0.5991,  ..., 0.5250, 0.0000, 1.5034],\n",
      "        [0.1987, 0.3928, 0.4812,  ..., 0.4017, 0.0000, 0.2987]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.5734, 0.4755, 0.3968,  ..., 0.4947, 0.0000, 0.6520],\n",
      "        [0.6383, 0.3918, 0.2451,  ..., 0.2708, 0.0000, 0.6532],\n",
      "        [0.6364, 0.3002, 0.3141,  ..., 0.1894, 0.0000, 0.4636],\n",
      "        ...,\n",
      "        [0.3879, 0.4116, 0.3650,  ..., 0.3474, 0.0000, 0.3610],\n",
      "        [0.4665, 0.8628, 0.5798,  ..., 0.6113, 0.0000, 0.5245],\n",
      "        [1.0292, 0.6330, 0.3372,  ..., 0.2015, 0.0000, 0.8880]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.9393, 0.1657, 0.0542,  ..., 0.1274, 0.0000, 0.7546],\n",
      "        [0.3566, 1.1457, 0.7136,  ..., 0.7401, 0.0000, 0.2991],\n",
      "        [4.2133, 3.8277, 2.5089,  ..., 2.7566, 0.0000, 4.4882],\n",
      "        ...,\n",
      "        [0.9294, 0.5545, 0.1812,  ..., 0.1523, 0.0000, 0.8515],\n",
      "        [0.6492, 0.5571, 0.2724,  ..., 0.2473, 0.0000, 0.4997],\n",
      "        [0.1836, 1.1136, 0.8860,  ..., 0.7405, 0.0000, 0.3864]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.3548, 1.4650, 0.9175,  ..., 0.9378, 0.0000, 1.3748],\n",
      "        [0.8780, 0.2060, 0.0684,  ..., 0.2425, 0.0000, 1.0417],\n",
      "        [5.0388, 2.7224, 1.7171,  ..., 1.4717, 0.0000, 5.2726],\n",
      "        ...,\n",
      "        [1.0664, 0.0000, 0.0230,  ..., 0.0631, 0.0000, 1.1388],\n",
      "        [0.9757, 0.9195, 0.6523,  ..., 0.7947, 0.0000, 1.0591],\n",
      "        [0.4926, 0.7537, 0.6526,  ..., 0.4928, 0.0000, 0.5237]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.9653, 1.0136, 0.5757,  ..., 0.8146, 0.0000, 1.0059],\n",
      "        [0.6940, 0.4519, 0.2035,  ..., 0.2733, 0.0000, 0.7874],\n",
      "        [0.6008, 0.4973, 0.1843,  ..., 0.0667, 0.0000, 0.7606],\n",
      "        ...,\n",
      "        [1.1129, 0.9156, 0.3675,  ..., 0.4848, 0.0000, 1.5514],\n",
      "        [1.4827, 1.9146, 1.5247,  ..., 1.2555, 0.0000, 1.5845],\n",
      "        [0.6579, 0.5972, 0.4828,  ..., 0.3875, 0.0000, 0.7675]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.7051, 0.5153, 0.2886,  ..., 0.2148, 0.0000, 0.8447],\n",
      "        [0.4669, 0.3359, 0.2858,  ..., 0.2120, 0.0000, 0.5756],\n",
      "        [0.3229, 0.8647, 0.5865,  ..., 0.5678, 0.0000, 0.3056],\n",
      "        ...,\n",
      "        [0.3807, 0.9647, 0.4482,  ..., 0.4984, 0.0000, 0.5270],\n",
      "        [0.5645, 0.9444, 0.6483,  ..., 0.4287, 0.0000, 0.6074],\n",
      "        [1.3168, 0.5770, 0.4122,  ..., 0.4525, 0.0000, 1.1908]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.6085, 0.7154, 0.6276,  ..., 0.5280, 0.0000, 0.4820],\n",
      "        [0.7029, 0.4303, 0.3540,  ..., 0.1088, 0.0000, 0.7309],\n",
      "        [0.6967, 0.3500, 0.3079,  ..., 0.2906, 0.0000, 0.6400],\n",
      "        ...,\n",
      "        [0.5931, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.8808],\n",
      "        [0.3181, 0.6142, 0.5580,  ..., 0.3815, 0.0000, 0.3192],\n",
      "        [1.5340, 0.9575, 0.4735,  ..., 0.5476, 0.0000, 1.3910]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.4235, 1.4247, 1.1006,  ..., 1.1716, 0.0000, 1.6168],\n",
      "        [0.7855, 0.8154, 0.6107,  ..., 0.6482, 0.0000, 0.6901],\n",
      "        [0.4444, 0.3974, 0.1286,  ..., 0.3369, 0.0000, 0.4803],\n",
      "        ...,\n",
      "        [0.3904, 0.1458, 0.0855,  ..., 0.0477, 0.0000, 0.3601],\n",
      "        [0.6820, 0.1804, 0.1773,  ..., 0.1052, 0.0000, 0.7100],\n",
      "        [0.9917, 0.3941, 0.1461,  ..., 0.2252, 0.0000, 0.7005]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0073, 0.6268, 0.5396,  ..., 0.5713, 0.0000, 0.9539],\n",
      "        [0.5866, 0.1138, 0.0655,  ..., 0.0447, 0.0000, 0.5232],\n",
      "        [0.5041, 1.0157, 0.7563,  ..., 0.6519, 0.0000, 0.4972],\n",
      "        ...,\n",
      "        [0.3127, 0.0000, 0.0544,  ..., 0.0585, 0.0000, 0.3267],\n",
      "        [1.1276, 0.2527, 0.0521,  ..., 0.2040, 0.0000, 1.1507],\n",
      "        [1.6419, 0.4435, 0.1558,  ..., 0.2473, 0.0000, 1.6821]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3322, 0.4219, 0.3294,  ..., 0.2420, 0.0000, 0.2307],\n",
      "        [1.7597, 0.2878, 0.0675,  ..., 0.1066, 0.0000, 1.7253],\n",
      "        [1.7806, 0.7159, 0.4664,  ..., 0.4148, 0.0000, 2.1069],\n",
      "        ...,\n",
      "        [0.9926, 0.7122, 0.4558,  ..., 0.5135, 0.0000, 0.9895],\n",
      "        [0.5100, 0.4424, 0.2973,  ..., 0.3389, 0.0000, 0.7133],\n",
      "        [0.6747, 0.7650, 0.4364,  ..., 0.3364, 0.0000, 0.7146]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Epoch #3\tTrain Loss: 0.287\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5422, 0.6915, 0.4204,  ..., 0.4287, 0.0000, 1.6039],\n",
      "        [1.0267, 0.4466, 0.2896,  ..., 0.3004, 0.0000, 1.2005],\n",
      "        [0.9463, 0.4611, 0.2844,  ..., 0.2864, 0.0000, 0.9968],\n",
      "        ...,\n",
      "        [1.0309, 0.0000, 0.0336,  ..., 0.0000, 0.0000, 0.8959],\n",
      "        [1.0178, 0.4650, 0.0000,  ..., 0.0000, 0.0000, 0.8413],\n",
      "        [1.7521, 1.1108, 0.6548,  ..., 0.6185, 0.0000, 1.7208]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3340, 0.9896, 0.6973,  ..., 0.7706, 0.0000, 0.4393],\n",
      "        [1.5384, 0.6879, 0.6774,  ..., 0.4364, 0.0000, 1.5973],\n",
      "        [0.6596, 1.0271, 0.7781,  ..., 0.7607, 0.0000, 0.7474],\n",
      "        ...,\n",
      "        [3.2085, 1.4684, 0.8448,  ..., 0.6497, 0.0000, 3.3621],\n",
      "        [1.8658, 0.2941, 0.1667,  ..., 0.0000, 0.0000, 1.8073],\n",
      "        [0.4863, 0.2259, 0.1601,  ..., 0.0628, 0.0000, 0.4377]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1690, 0.5480, 0.2895,  ..., 0.4943, 0.0000, 1.2509],\n",
      "        [2.3865, 2.2328, 1.3612,  ..., 1.0264, 0.0000, 2.5111],\n",
      "        [1.0675, 0.5961, 0.4743,  ..., 0.3134, 0.0000, 1.0212],\n",
      "        ...,\n",
      "        [0.4901, 0.6512, 0.4369,  ..., 0.5534, 0.0000, 0.4920],\n",
      "        [0.9836, 0.5576, 0.4098,  ..., 0.3048, 0.0000, 0.8181],\n",
      "        [1.0225, 0.1713, 0.0000,  ..., 0.0000, 0.0000, 0.9598]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1806, 0.1865, 0.1206,  ..., 0.0252, 0.0000, 1.0384],\n",
      "        [0.4787, 0.0720, 0.0566,  ..., 0.0000, 0.0000, 0.3951],\n",
      "        [1.3564, 0.9903, 0.5650,  ..., 0.5100, 0.0000, 1.6716],\n",
      "        ...,\n",
      "        [0.9367, 0.7347, 0.7386,  ..., 0.5274, 0.0000, 0.9097],\n",
      "        [1.3581, 1.1238, 0.6964,  ..., 0.6184, 0.0000, 1.7530],\n",
      "        [0.2687, 0.4202, 0.5049,  ..., 0.4192, 0.0000, 0.3797]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.6231, 0.5548, 0.4799,  ..., 0.5657, 0.0000, 0.7245],\n",
      "        [0.6884, 0.4545, 0.3100,  ..., 0.3375, 0.0000, 0.7256],\n",
      "        [0.7352, 0.3321, 0.3448,  ..., 0.1974, 0.0000, 0.5759],\n",
      "        ...,\n",
      "        [0.4112, 0.4998, 0.4539,  ..., 0.4324, 0.0000, 0.4037],\n",
      "        [0.5257, 0.9799, 0.6938,  ..., 0.7186, 0.0000, 0.6096],\n",
      "        [1.1164, 0.7107, 0.4139,  ..., 0.2816, 0.0000, 1.0036]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0288, 0.1830, 0.0771,  ..., 0.1460, 0.0000, 0.8591],\n",
      "        [0.4195, 1.2514, 0.8131,  ..., 0.8326, 0.0000, 0.3885],\n",
      "        [4.8608, 3.9659, 2.6240,  ..., 2.8048, 0.0000, 5.2289],\n",
      "        ...,\n",
      "        [1.0341, 0.5732, 0.2030,  ..., 0.1712, 0.0000, 0.9711],\n",
      "        [0.6869, 0.6344, 0.3549,  ..., 0.3252, 0.0000, 0.5567],\n",
      "        [0.2910, 1.1504, 0.9078,  ..., 0.7619, 0.0000, 0.5123]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5288, 1.5535, 0.9999,  ..., 1.0116, 0.0000, 1.5844],\n",
      "        [0.9454, 0.2468, 0.1140,  ..., 0.2874, 0.0000, 1.1252],\n",
      "        [5.5330, 2.9681, 1.9577,  ..., 1.6847, 0.0000, 5.8751],\n",
      "        ...,\n",
      "        [1.1576, 0.0000, 0.0439,  ..., 0.0878, 0.0000, 1.2469],\n",
      "        [1.1152, 0.9550, 0.6866,  ..., 0.8142, 0.0000, 1.2184],\n",
      "        [0.5740, 0.7962, 0.6864,  ..., 0.5188, 0.0000, 0.6205]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0752, 1.0767, 0.6334,  ..., 0.8513, 0.0000, 1.1388],\n",
      "        [0.7835, 0.4787, 0.2328,  ..., 0.2974, 0.0000, 0.8926],\n",
      "        [0.6121, 0.6000, 0.2937,  ..., 0.1799, 0.0000, 0.7948],\n",
      "        ...,\n",
      "        [1.2488, 0.9592, 0.4031,  ..., 0.5052, 0.0000, 1.7088],\n",
      "        [1.5907, 2.0628, 1.6731,  ..., 1.3854, 0.0000, 1.7349],\n",
      "        [0.6971, 0.6859, 0.5719,  ..., 0.4780, 0.0000, 0.8309]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.8542, 0.4859, 0.2544,  ..., 0.1721, 0.0000, 1.0047],\n",
      "        [0.5054, 0.3481, 0.2959,  ..., 0.2213, 0.0000, 0.6218],\n",
      "        [0.3579, 0.9349, 0.6566,  ..., 0.6390, 0.0000, 0.3556],\n",
      "        ...,\n",
      "        [0.4999, 0.9528, 0.4350,  ..., 0.4632, 0.0000, 0.6549],\n",
      "        [0.5862, 1.0571, 0.7619,  ..., 0.5479, 0.0000, 0.6531],\n",
      "        [1.4192, 0.6422, 0.4827,  ..., 0.5258, 0.0000, 1.3179]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.6829, 0.7437, 0.6467,  ..., 0.5502, 0.0000, 0.5699],\n",
      "        [0.7429, 0.4870, 0.4069,  ..., 0.1720, 0.0000, 0.7879],\n",
      "        [0.7715, 0.3649, 0.3169,  ..., 0.3010, 0.0000, 0.7266],\n",
      "        ...,\n",
      "        [0.6259, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.9245],\n",
      "        [0.3716, 0.6349, 0.5720,  ..., 0.3960, 0.0000, 0.3813],\n",
      "        [1.6093, 1.0537, 0.5784,  ..., 0.6482, 0.0000, 1.4949]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5398, 1.4890, 1.1523,  ..., 1.2204, 0.0000, 1.7583],\n",
      "        [0.8315, 0.8710, 0.6644,  ..., 0.6974, 0.0000, 0.7538],\n",
      "        [0.4254, 0.4697, 0.2076,  ..., 0.4119, 0.0000, 0.4727],\n",
      "        ...,\n",
      "        [0.4427, 0.1356, 0.0711,  ..., 0.0348, 0.0000, 0.4165],\n",
      "        [0.7070, 0.2351, 0.2330,  ..., 0.1627, 0.0000, 0.7482],\n",
      "        [1.0017, 0.4689, 0.2263,  ..., 0.3055, 0.0000, 0.7253]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0370, 0.6996, 0.6132,  ..., 0.6382, 0.0000, 1.0047],\n",
      "        [0.6047, 0.1445, 0.0985,  ..., 0.0778, 0.0000, 0.5499],\n",
      "        [0.5412, 1.0784, 0.8167,  ..., 0.7081, 0.0000, 0.5507],\n",
      "        ...,\n",
      "        [0.3396, 0.0000, 0.0629,  ..., 0.0641, 0.0000, 0.3565],\n",
      "        [1.2118, 0.2511, 0.0582,  ..., 0.2053, 0.0000, 1.2447],\n",
      "        [1.6926, 0.5059, 0.2269,  ..., 0.3118, 0.0000, 1.7504]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3586, 0.4399, 0.3426,  ..., 0.2571, 0.0000, 0.2625],\n",
      "        [1.8033, 0.3513, 0.1391,  ..., 0.1742, 0.0000, 1.7900],\n",
      "        [1.8721, 0.7563, 0.5030,  ..., 0.4398, 0.0000, 2.2175],\n",
      "        ...,\n",
      "        [1.0324, 0.7611, 0.5038,  ..., 0.5581, 0.0000, 1.0446],\n",
      "        [0.5369, 0.4759, 0.3300,  ..., 0.3678, 0.0000, 0.7483],\n",
      "        [0.6994, 0.8225, 0.4961,  ..., 0.3967, 0.0000, 0.7524]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Epoch #4\tTrain Loss: 0.258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5822, 0.7551, 0.4779,  ..., 0.4881, 0.0000, 1.6629],\n",
      "        [1.0903, 0.4561, 0.2977,  ..., 0.2983, 0.0000, 1.2727],\n",
      "        [0.9762, 0.5037, 0.3253,  ..., 0.3241, 0.0000, 1.0365],\n",
      "        ...,\n",
      "        [1.0664, 0.0000, 0.0753,  ..., 0.0373, 0.0000, 0.9437],\n",
      "        [1.0787, 0.4775, 0.0000,  ..., 0.0000, 0.0000, 0.9113],\n",
      "        [1.8549, 1.1391, 0.6825,  ..., 0.6355, 0.0000, 1.8402]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3646, 1.0295, 0.7342,  ..., 0.8085, 0.0000, 0.4816],\n",
      "        [1.6555, 0.6766, 0.6587,  ..., 0.4099, 0.0000, 1.7260],\n",
      "        [0.7153, 1.0605, 0.8124,  ..., 0.7866, 0.0000, 0.8105],\n",
      "        ...,\n",
      "        [3.3616, 1.5046, 0.8831,  ..., 0.6769, 0.0000, 3.5400],\n",
      "        [1.9127, 0.3596, 0.2315,  ..., 0.0356, 0.0000, 1.8743],\n",
      "        [0.5184, 0.2378, 0.1723,  ..., 0.0746, 0.0000, 0.4739]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2050, 0.5798, 0.3214,  ..., 0.5197, 0.0000, 1.2955],\n",
      "        [2.5066, 2.2782, 1.4006,  ..., 1.0507, 0.0000, 2.6509],\n",
      "        [1.1409, 0.5883, 0.4595,  ..., 0.2975, 0.0000, 1.1025],\n",
      "        ...,\n",
      "        [0.5378, 0.6667, 0.4471,  ..., 0.5627, 0.0000, 0.5460],\n",
      "        [1.0708, 0.5322, 0.3802,  ..., 0.2660, 0.0000, 0.9097],\n",
      "        [1.0481, 0.1924, 0.0000,  ..., 0.0000, 0.0000, 0.9891]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2402, 0.1855, 0.1224,  ..., 0.0184, 0.0000, 1.1033],\n",
      "        [0.4821, 0.0990, 0.0845,  ..., 0.0000, 0.0000, 0.4039],\n",
      "        [1.4569, 0.9802, 0.5553,  ..., 0.4909, 0.0000, 1.7777],\n",
      "        ...,\n",
      "        [1.0091, 0.7275, 0.7292,  ..., 0.5160, 0.0000, 0.9903],\n",
      "        [1.4143, 1.1699, 0.7433,  ..., 0.6599, 0.0000, 1.8241],\n",
      "        [0.2984, 0.4253, 0.5106,  ..., 0.4206, 0.0000, 0.4126]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.6143, 0.6096, 0.5350,  ..., 0.6163, 0.0000, 0.7258],\n",
      "        [0.7052, 0.4795, 0.3330,  ..., 0.3617, 0.0000, 0.7497],\n",
      "        [0.7977, 0.3226, 0.3314,  ..., 0.1695, 0.0000, 0.6405],\n",
      "        ...,\n",
      "        [0.4055, 0.5452, 0.4996,  ..., 0.4716, 0.0000, 0.4055],\n",
      "        [0.5811, 0.9861, 0.6961,  ..., 0.7139, 0.0000, 0.6698],\n",
      "        [1.1247, 0.7547, 0.4557,  ..., 0.3204, 0.0000, 1.0216]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0359, 0.2175, 0.1149,  ..., 0.1809, 0.0000, 0.8728],\n",
      "        [0.4773, 1.2595, 0.8159,  ..., 0.8285, 0.0000, 0.4508],\n",
      "        [5.1168, 3.9908, 2.6288,  ..., 2.7761, 0.0000, 5.5137],\n",
      "        ...,\n",
      "        [1.0683, 0.5834, 0.2140,  ..., 0.1825, 0.0000, 1.0108],\n",
      "        [0.7028, 0.6624, 0.3847,  ..., 0.3529, 0.0000, 0.5791],\n",
      "        [0.3628, 1.1285, 0.8795,  ..., 0.7258, 0.0000, 0.5881]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5613, 1.6106, 1.0540,  ..., 1.0690, 0.0000, 1.6311],\n",
      "        [0.9533, 0.2812, 0.1483,  ..., 0.3232, 0.0000, 1.1399],\n",
      "        [5.7190, 3.0438, 2.0142,  ..., 1.7257, 0.0000, 6.0983],\n",
      "        ...,\n",
      "        [1.1880, 0.0166, 0.0568,  ..., 0.1011, 0.0000, 1.2829],\n",
      "        [1.1495, 0.9790, 0.7110,  ..., 0.8337, 0.0000, 1.2611],\n",
      "        [0.6304, 0.7829, 0.6658,  ..., 0.4926, 0.0000, 0.6803]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1375, 1.0797, 0.6286,  ..., 0.8359, 0.0000, 1.2067],\n",
      "        [0.8118, 0.4950, 0.2495,  ..., 0.3105, 0.0000, 0.9264],\n",
      "        [0.6238, 0.6311, 0.3284,  ..., 0.2083, 0.0000, 0.8129],\n",
      "        ...,\n",
      "        [1.2861, 0.9863, 0.4238,  ..., 0.5218, 0.0000, 1.7535],\n",
      "        [1.6845, 2.0572, 1.6654,  ..., 1.3589, 0.0000, 1.8399],\n",
      "        [0.7181, 0.7107, 0.5974,  ..., 0.4992, 0.0000, 0.8592]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.9230, 0.4596, 0.2274,  ..., 0.1363, 0.0000, 1.0747],\n",
      "        [0.5195, 0.3527, 0.2973,  ..., 0.2223, 0.0000, 0.6376],\n",
      "        [0.3864, 0.9502, 0.6709,  ..., 0.6503, 0.0000, 0.3880],\n",
      "        ...,\n",
      "        [0.5278, 0.9635, 0.4459,  ..., 0.4622, 0.0000, 0.6864],\n",
      "        [0.5957, 1.0966, 0.8023,  ..., 0.5858, 0.0000, 0.6698],\n",
      "        [1.4497, 0.6769, 0.5215,  ..., 0.5624, 0.0000, 1.3576]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.7200, 0.7477, 0.6455,  ..., 0.5525, 0.0000, 0.6114],\n",
      "        [0.7613, 0.5072, 0.4272,  ..., 0.1889, 0.0000, 0.8106],\n",
      "        [0.8005, 0.3646, 0.3136,  ..., 0.2971, 0.0000, 0.7582],\n",
      "        ...,\n",
      "        [0.6001, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.9047],\n",
      "        [0.3980, 0.6403, 0.5713,  ..., 0.3950, 0.0000, 0.4098],\n",
      "        [1.6329, 1.0936, 0.6201,  ..., 0.6883, 0.0000, 1.5279]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5546, 1.5399, 1.1929,  ..., 1.2666, 0.0000, 1.7800],\n",
      "        [0.8313, 0.9086, 0.6976,  ..., 0.7296, 0.0000, 0.7600],\n",
      "        [0.4232, 0.4922, 0.2333,  ..., 0.4331, 0.0000, 0.4730],\n",
      "        ...,\n",
      "        [0.4603, 0.1352, 0.0699,  ..., 0.0330, 0.0000, 0.4357],\n",
      "        [0.7212, 0.2505, 0.2486,  ..., 0.1765, 0.0000, 0.7660],\n",
      "        [0.9859, 0.5188, 0.2793,  ..., 0.3583, 0.0000, 0.7148]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0482, 0.7286, 0.6409,  ..., 0.6551, 0.0000, 1.0234],\n",
      "        [0.6094, 0.1589, 0.1138,  ..., 0.0908, 0.0000, 0.5562],\n",
      "        [0.5533, 1.1023, 0.8413,  ..., 0.7305, 0.0000, 0.5672],\n",
      "        ...,\n",
      "        [0.3543, 0.0000, 0.0616,  ..., 0.0585, 0.0000, 0.3712],\n",
      "        [1.2301, 0.2641, 0.0719,  ..., 0.2201, 0.0000, 1.2670],\n",
      "        [1.7101, 0.5327, 0.2556,  ..., 0.3405, 0.0000, 1.7730]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3722, 0.4451, 0.3442,  ..., 0.2613, 0.0000, 0.2776],\n",
      "        [1.8261, 0.3693, 0.1600,  ..., 0.1907, 0.0000, 1.8196],\n",
      "        [1.9263, 0.7509, 0.4930,  ..., 0.4172, 0.0000, 2.2759],\n",
      "        ...,\n",
      "        [1.0501, 0.7769, 0.5161,  ..., 0.5681, 0.0000, 1.0654],\n",
      "        [0.5560, 0.4785, 0.3304,  ..., 0.3665, 0.0000, 0.7685],\n",
      "        [0.7247, 0.8276, 0.5024,  ..., 0.3994, 0.0000, 0.7809]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Epoch #5\tTrain Loss: 0.246\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5931, 0.7814, 0.4968,  ..., 0.5083, 0.0000, 1.6797],\n",
      "        [1.1259, 0.4490, 0.2870,  ..., 0.2854, 0.0000, 1.3100],\n",
      "        [0.9865, 0.5218, 0.3412,  ..., 0.3377, 0.0000, 1.0502],\n",
      "        ...,\n",
      "        [1.0591, 0.0000, 0.1188,  ..., 0.0870, 0.0000, 0.9431],\n",
      "        [1.0850, 0.5001, 0.0000,  ..., 0.0000, 0.0000, 0.9218],\n",
      "        [1.8988, 1.1466, 0.6864,  ..., 0.6347, 0.0000, 1.8884]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3803, 1.0428, 0.7442,  ..., 0.8208, 0.0000, 0.5006],\n",
      "        [1.6800, 0.6907, 0.6673,  ..., 0.4149, 0.0000, 1.7546],\n",
      "        [0.7159, 1.0985, 0.8498,  ..., 0.8227, 0.0000, 0.8149],\n",
      "        ...,\n",
      "        [3.3990, 1.5379, 0.9109,  ..., 0.7036, 0.0000, 3.5848],\n",
      "        [1.9123, 0.4013, 0.2722,  ..., 0.0746, 0.0000, 1.8817],\n",
      "        [0.5225, 0.2522, 0.1883,  ..., 0.0885, 0.0000, 0.4804]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2205, 0.5924, 0.3325,  ..., 0.5274, 0.0000, 1.3129],\n",
      "        [2.5534, 2.2984, 1.4144,  ..., 1.0513, 0.0000, 2.7034],\n",
      "        [1.1776, 0.5779, 0.4390,  ..., 0.2826, 0.0000, 1.1416],\n",
      "        ...,\n",
      "        [0.5726, 0.6599, 0.4363,  ..., 0.5479, 0.0000, 0.5802],\n",
      "        [1.0879, 0.5389, 0.3821,  ..., 0.2685, 0.0000, 0.9293],\n",
      "        [1.0502, 0.2125, 0.0000,  ..., 0.0000, 0.0000, 0.9922]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2543e+00, 1.9817e-01, 1.3560e-01,  ..., 2.6168e-02, 0.0000e+00,\n",
      "         1.1196e+00],\n",
      "        [4.7608e-01, 1.1859e-01, 1.0454e-01,  ..., 1.1882e-03, 0.0000e+00,\n",
      "         4.0035e-01],\n",
      "        [1.4941e+00, 9.8068e-01, 5.5466e-01,  ..., 4.8506e-01, 0.0000e+00,\n",
      "         1.8173e+00],\n",
      "        ...,\n",
      "        [1.0354e+00, 7.2597e-01, 7.2593e-01,  ..., 5.1189e-01, 0.0000e+00,\n",
      "         1.0196e+00],\n",
      "        [1.4241e+00, 1.2004e+00, 7.7408e-01,  ..., 6.8650e-01, 0.0000e+00,\n",
      "         1.8390e+00],\n",
      "        [3.2376e-01, 4.1398e-01, 4.9916e-01,  ..., 4.0517e-01, 0.0000e+00,\n",
      "         4.3846e-01]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.6144, 0.6279, 0.5519,  ..., 0.6305, 0.0000, 0.7290],\n",
      "        [0.7265, 0.4750, 0.3239,  ..., 0.3538, 0.0000, 0.7726],\n",
      "        [0.8190, 0.3247, 0.3316,  ..., 0.1605, 0.0000, 0.6624],\n",
      "        ...,\n",
      "        [0.4152, 0.5524, 0.5038,  ..., 0.4686, 0.0000, 0.4172],\n",
      "        [0.6108, 0.9816, 0.6907,  ..., 0.7049, 0.0000, 0.7006],\n",
      "        [1.1283, 0.7694, 0.4666,  ..., 0.3263, 0.0000, 1.0279]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0307, 0.2408, 0.1380,  ..., 0.2017, 0.0000, 0.8700],\n",
      "        [0.5073, 1.2582, 0.8094,  ..., 0.8209, 0.0000, 0.4816],\n",
      "        [5.1965, 4.0228, 2.6417,  ..., 2.7707, 0.0000, 5.6038],\n",
      "        ...,\n",
      "        [1.0609, 0.6094, 0.2394,  ..., 0.2104, 0.0000, 1.0067],\n",
      "        [0.7117, 0.6753, 0.3962,  ..., 0.3636, 0.0000, 0.5905],\n",
      "        [0.3891, 1.1236, 0.8715,  ..., 0.7099, 0.0000, 0.6165]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5640, 1.6429, 1.0786,  ..., 1.1016, 0.0000, 1.6391],\n",
      "        [0.9474, 0.3065, 0.1698,  ..., 0.3473, 0.0000, 1.1374],\n",
      "        [5.7573, 3.1062, 2.0554,  ..., 1.7571, 0.0000, 6.1523],\n",
      "        ...,\n",
      "        [1.2059, 0.0186, 0.0564,  ..., 0.1011, 0.0000, 1.3017],\n",
      "        [1.1369, 1.0135, 0.7459,  ..., 0.8660, 0.0000, 1.2536],\n",
      "        [0.6557, 0.7735, 0.6515,  ..., 0.4740, 0.0000, 0.7063]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1651, 1.0785, 0.6190,  ..., 0.8219, 0.0000, 1.2356],\n",
      "        [0.8280, 0.4984, 0.2492,  ..., 0.3066, 0.0000, 0.9437],\n",
      "        [0.6282, 0.6465, 0.3451,  ..., 0.2193, 0.0000, 0.8197],\n",
      "        ...,\n",
      "        [1.2944, 1.0032, 0.4350,  ..., 0.5323, 0.0000, 1.7645],\n",
      "        [1.7171, 2.0575, 1.6656,  ..., 1.3421, 0.0000, 1.8772],\n",
      "        [0.7347, 0.7123, 0.5991,  ..., 0.4960, 0.0000, 0.8776]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.9389, 0.4619, 0.2303,  ..., 0.1336, 0.0000, 1.0915],\n",
      "        [0.5231, 0.3573, 0.2986,  ..., 0.2242, 0.0000, 0.6413],\n",
      "        [0.4028, 0.9538, 0.6724,  ..., 0.6504, 0.0000, 0.4052],\n",
      "        ...,\n",
      "        [0.5170, 0.9887, 0.4702,  ..., 0.4803, 0.0000, 0.6767],\n",
      "        [0.5893, 1.1235, 0.8307,  ..., 0.6111, 0.0000, 0.6667],\n",
      "        [1.4652, 0.6874, 0.5325,  ..., 0.5699, 0.0000, 1.3750]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.7356, 0.7500, 0.6431,  ..., 0.5531, 0.0000, 0.6281],\n",
      "        [0.7764, 0.5088, 0.4274,  ..., 0.1842, 0.0000, 0.8261],\n",
      "        [0.8059, 0.3691, 0.3157,  ..., 0.2972, 0.0000, 0.7643],\n",
      "        ...,\n",
      "        [0.5826, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.8914],\n",
      "        [0.4121, 0.6403, 0.5664,  ..., 0.3905, 0.0000, 0.4241],\n",
      "        [1.6490, 1.1048, 0.6296,  ..., 0.6963, 0.0000, 1.5463]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5383, 1.5787, 1.2213,  ..., 1.3035, 0.0000, 1.7660],\n",
      "        [0.8350, 0.9198, 0.7030,  ..., 0.7334, 0.0000, 0.7656],\n",
      "        [0.4243, 0.4982, 0.2412,  ..., 0.4365, 0.0000, 0.4743],\n",
      "        ...,\n",
      "        [0.4582, 0.1442, 0.0786,  ..., 0.0410, 0.0000, 0.4344],\n",
      "        [0.7244, 0.2588, 0.2571,  ..., 0.1822, 0.0000, 0.7701],\n",
      "        [0.9794, 0.5404, 0.3009,  ..., 0.3792, 0.0000, 0.7092]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0564, 0.7358, 0.6446,  ..., 0.6458, 0.0000, 1.0339],\n",
      "        [0.6150, 0.1630, 0.1176,  ..., 0.0924, 0.0000, 0.5615],\n",
      "        [0.5562, 1.1143, 0.8521,  ..., 0.7423, 0.0000, 0.5714],\n",
      "        ...,\n",
      "        [0.3583, 0.0000, 0.0635,  ..., 0.0568, 0.0000, 0.3749],\n",
      "        [1.2348, 0.2714, 0.0756,  ..., 0.2274, 0.0000, 1.2733],\n",
      "        [1.7170, 0.5466, 0.2673,  ..., 0.3527, 0.0000, 1.7813]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3749, 0.4510, 0.3469,  ..., 0.2668, 0.0000, 0.2809],\n",
      "        [1.8293, 0.3826, 0.1717,  ..., 0.1974, 0.0000, 1.8252],\n",
      "        [1.9539, 0.7424, 0.4779,  ..., 0.3926, 0.0000, 2.3041],\n",
      "        ...,\n",
      "        [1.0551, 0.7841, 0.5188,  ..., 0.5700, 0.0000, 1.0709],\n",
      "        [0.5703, 0.4720, 0.3201,  ..., 0.3563, 0.0000, 0.7825],\n",
      "        [0.7359, 0.8285, 0.5029,  ..., 0.3978, 0.0000, 0.7930]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Epoch #6\tTrain Loss: 0.241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6002, 0.7868, 0.4924,  ..., 0.5049, 0.0000, 1.6879],\n",
      "        [1.1453, 0.4412, 0.2733,  ..., 0.2727, 0.0000, 1.3293],\n",
      "        [0.9898, 0.5299, 0.3460,  ..., 0.3396, 0.0000, 1.0538],\n",
      "        ...,\n",
      "        [1.0574, 0.0000, 0.1356,  ..., 0.1049, 0.0000, 0.9431],\n",
      "        [1.0862, 0.5114, 0.0000,  ..., 0.0000, 0.0000, 0.9248],\n",
      "        [1.9164, 1.1512, 0.6860,  ..., 0.6324, 0.0000, 1.9070]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3874, 1.0477, 0.7456,  ..., 0.8249, 0.0000, 0.5086],\n",
      "        [1.6830, 0.7020, 0.6716,  ..., 0.4175, 0.0000, 1.7581],\n",
      "        [0.7200, 1.1123, 0.8617,  ..., 0.8328, 0.0000, 0.8200],\n",
      "        ...,\n",
      "        [3.4128, 1.5544, 0.9152,  ..., 0.7078, 0.0000, 3.6009],\n",
      "        [1.9216, 0.4094, 0.2774,  ..., 0.0754, 0.0000, 1.8922],\n",
      "        [0.5220, 0.2611, 0.1978,  ..., 0.0953, 0.0000, 0.4803]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2250, 0.6005, 0.3370,  ..., 0.5297, 0.0000, 1.3176],\n",
      "        [2.5897, 2.2898, 1.3954,  ..., 1.0216, 0.0000, 2.7396],\n",
      "        [1.1911, 0.5763, 0.4250,  ..., 0.2778, 0.0000, 1.1561],\n",
      "        ...,\n",
      "        [0.5910, 0.6543, 0.4273,  ..., 0.5356, 0.0000, 0.5973],\n",
      "        [1.0931, 0.5446, 0.3806,  ..., 0.2700, 0.0000, 0.9352],\n",
      "        [1.0547, 0.2193, 0.0000,  ..., 0.0000, 0.0000, 0.9961]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2582, 0.2079, 0.1436,  ..., 0.0296, 0.0000, 1.1241],\n",
      "        [0.4760, 0.1250, 0.1101,  ..., 0.0072, 0.0000, 0.4007],\n",
      "        [1.5010, 0.9909, 0.5608,  ..., 0.4873, 0.0000, 1.8245],\n",
      "        ...,\n",
      "        [1.0389, 0.7316, 0.7295,  ..., 0.5136, 0.0000, 1.0243],\n",
      "        [1.4305, 1.2126, 0.7835,  ..., 0.6923, 0.0000, 1.8469],\n",
      "        [0.3327, 0.4104, 0.4958,  ..., 0.3990, 0.0000, 0.4475]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.6044, 0.6464, 0.5688,  ..., 0.6462, 0.0000, 0.7210],\n",
      "        [0.7352, 0.4727, 0.3156,  ..., 0.3483, 0.0000, 0.7815],\n",
      "        [0.8283, 0.3285, 0.3322,  ..., 0.1527, 0.0000, 0.6718],\n",
      "        ...,\n",
      "        [0.4154, 0.5612, 0.5079,  ..., 0.4683, 0.0000, 0.4188],\n",
      "        [0.6219, 0.9824, 0.6909,  ..., 0.7028, 0.0000, 0.7118],\n",
      "        [1.1346, 0.7699, 0.4620,  ..., 0.3166, 0.0000, 1.0344]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0256, 0.2551, 0.1500,  ..., 0.2120, 0.0000, 0.8656],\n",
      "        [0.5254, 1.2547, 0.7995,  ..., 0.8121, 0.0000, 0.5002],\n",
      "        [5.2286, 4.0419, 2.6370,  ..., 2.7494, 0.0000, 5.6401],\n",
      "        ...,\n",
      "        [1.0528, 0.6256, 0.2527,  ..., 0.2249, 0.0000, 0.9993],\n",
      "        [0.7143, 0.6847, 0.4024,  ..., 0.3680, 0.0000, 0.5941],\n",
      "        [0.4024, 1.1217, 0.8657,  ..., 0.6952, 0.0000, 0.6308]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5660, 1.6564, 1.0793,  ..., 1.1116, 0.0000, 1.6429],\n",
      "        [0.9436, 0.3207, 0.1780,  ..., 0.3574, 0.0000, 1.1351],\n",
      "        [5.7920, 3.1153, 2.0323,  ..., 1.7225, 0.0000, 6.1911],\n",
      "        ...,\n",
      "        [1.2139, 0.0204, 0.0556,  ..., 0.1001, 0.0000, 1.3097],\n",
      "        [1.1264, 1.0346, 0.7646,  ..., 0.8818, 0.0000, 1.2460],\n",
      "        [0.6643, 0.7717, 0.6449,  ..., 0.4642, 0.0000, 0.7154]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1714, 1.0852, 0.6168,  ..., 0.8165, 0.0000, 1.2430],\n",
      "        [0.8299, 0.5070, 0.2526,  ..., 0.3069, 0.0000, 0.9460],\n",
      "        [0.6360, 0.6489, 0.3469,  ..., 0.2156, 0.0000, 0.8279],\n",
      "        ...,\n",
      "        [1.2980, 1.0122, 0.4362,  ..., 0.5323, 0.0000, 1.7688],\n",
      "        [1.7469, 2.0421, 1.6461,  ..., 1.3060, 0.0000, 1.9087],\n",
      "        [0.7467, 0.7084, 0.5944,  ..., 0.4860, 0.0000, 0.8898]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.9437, 0.4663, 0.2340,  ..., 0.1326, 0.0000, 1.0967],\n",
      "        [0.5218, 0.3626, 0.3007,  ..., 0.2272, 0.0000, 0.6399],\n",
      "        [0.4015, 0.9650, 0.6811,  ..., 0.6594, 0.0000, 0.4047],\n",
      "        ...,\n",
      "        [0.5110, 1.0018, 0.4797,  ..., 0.4842, 0.0000, 0.6708],\n",
      "        [0.5822, 1.1404, 0.8482,  ..., 0.6243, 0.0000, 0.6612],\n",
      "        [1.4712, 0.6928, 0.5362,  ..., 0.5698, 0.0000, 1.3809]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.7412, 0.7548, 0.6424,  ..., 0.5548, 0.0000, 0.6342],\n",
      "        [0.7818, 0.5117, 0.4279,  ..., 0.1805, 0.0000, 0.8316],\n",
      "        [0.8046, 0.3739, 0.3172,  ..., 0.2954, 0.0000, 0.7636],\n",
      "        ...,\n",
      "        [0.5694, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.8799],\n",
      "        [0.4157, 0.6439, 0.5658,  ..., 0.3911, 0.0000, 0.4279],\n",
      "        [1.6545, 1.1138, 0.6347,  ..., 0.7009, 0.0000, 1.5528]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5289, 1.5974, 1.2268,  ..., 1.3189, 0.0000, 1.7569],\n",
      "        [0.8374, 0.9241, 0.7009,  ..., 0.7301, 0.0000, 0.7692],\n",
      "        [0.4189, 0.5061, 0.2503,  ..., 0.4421, 0.0000, 0.4691],\n",
      "        ...,\n",
      "        [0.4536, 0.1524, 0.0853,  ..., 0.0473, 0.0000, 0.4304],\n",
      "        [0.7261, 0.2628, 0.2603,  ..., 0.1823, 0.0000, 0.7720],\n",
      "        [0.9755, 0.5524, 0.3108,  ..., 0.3885, 0.0000, 0.7056]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0617, 0.7373, 0.6398,  ..., 0.6284, 0.0000, 1.0402],\n",
      "        [0.6191, 0.1655, 0.1188,  ..., 0.0921, 0.0000, 0.5653],\n",
      "        [0.5609, 1.1183, 0.8533,  ..., 0.7452, 0.0000, 0.5763],\n",
      "        ...,\n",
      "        [0.3592, 0.0000, 0.0657,  ..., 0.0563, 0.0000, 0.3755],\n",
      "        [1.2367, 0.2754, 0.0732,  ..., 0.2304, 0.0000, 1.2764],\n",
      "        [1.7216, 0.5544, 0.2705,  ..., 0.3552, 0.0000, 1.7859]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3741, 0.4573, 0.3494,  ..., 0.2722, 0.0000, 0.2804],\n",
      "        [1.8359, 0.3842, 0.1674,  ..., 0.1882, 0.0000, 1.8324],\n",
      "        [1.9675, 0.7397, 0.4671,  ..., 0.3739, 0.0000, 2.3185],\n",
      "        ...,\n",
      "        [1.0603, 0.7839, 0.5131,  ..., 0.5643, 0.0000, 1.0759],\n",
      "        [0.5786, 0.4674, 0.3105,  ..., 0.3478, 0.0000, 0.7901],\n",
      "        [0.7397, 0.8312, 0.5031,  ..., 0.3965, 0.0000, 0.7972]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Epoch #7\tTrain Loss: 0.238\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6011e+00, 7.9071e-01, 4.8357e-01,  ..., 4.9880e-01, 0.0000e+00,\n",
      "         1.6896e+00],\n",
      "        [1.1578e+00, 4.3506e-01, 2.5969e-01,  ..., 2.6106e-01, 0.0000e+00,\n",
      "         1.3417e+00],\n",
      "        [9.8879e-01, 5.3717e-01, 3.4848e-01,  ..., 3.3992e-01, 0.0000e+00,\n",
      "         1.0527e+00],\n",
      "        ...,\n",
      "        [1.0590e+00, 1.2945e-03, 1.4186e-01,  ..., 1.1100e-01, 0.0000e+00,\n",
      "         9.4542e-01],\n",
      "        [1.0899e+00, 5.1521e-01, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         9.2940e-01],\n",
      "        [1.9209e+00, 1.1593e+00, 6.8769e-01,  ..., 6.3281e-01, 0.0000e+00,\n",
      "         1.9122e+00]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3874, 1.0544, 0.7480,  ..., 0.8304, 0.0000, 0.5094],\n",
      "        [1.6879, 0.7046, 0.6644,  ..., 0.4094, 0.0000, 1.7629],\n",
      "        [0.7232, 1.1199, 0.8662,  ..., 0.8356, 0.0000, 0.8240],\n",
      "        ...,\n",
      "        [3.4228, 1.5612, 0.9041,  ..., 0.6975, 0.0000, 3.6124],\n",
      "        [1.9270, 0.4139, 0.2775,  ..., 0.0711, 0.0000, 1.8982],\n",
      "        [0.5207, 0.2682, 0.2042,  ..., 0.0993, 0.0000, 0.4791]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2266, 0.6067, 0.3377,  ..., 0.5296, 0.0000, 1.3191],\n",
      "        [2.6104, 2.2829, 1.3746,  ..., 0.9934, 0.0000, 2.7603],\n",
      "        [1.1997, 0.5749, 0.4090,  ..., 0.2721, 0.0000, 1.1651],\n",
      "        ...,\n",
      "        [0.5989, 0.6544, 0.4235,  ..., 0.5302, 0.0000, 0.6050],\n",
      "        [1.0994, 0.5451, 0.3720,  ..., 0.2648, 0.0000, 0.9417],\n",
      "        [1.0551, 0.2264, 0.0000,  ..., 0.0000, 0.0000, 0.9962]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2626, 0.2125, 0.1445,  ..., 0.0265, 0.0000, 1.1288],\n",
      "        [0.4759, 0.1292, 0.1129,  ..., 0.0104, 0.0000, 0.4009],\n",
      "        [1.5057, 0.9964, 0.5586,  ..., 0.4821, 0.0000, 1.8291],\n",
      "        ...,\n",
      "        [1.0378, 0.7365, 0.7313,  ..., 0.5134, 0.0000, 1.0241],\n",
      "        [1.4365, 1.2187, 0.7838,  ..., 0.6904, 0.0000, 1.8537],\n",
      "        [0.3374, 0.4079, 0.4926,  ..., 0.3941, 0.0000, 0.4524]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.5999, 0.6559, 0.5751,  ..., 0.6519, 0.0000, 0.7179],\n",
      "        [0.7342, 0.4775, 0.3137,  ..., 0.3498, 0.0000, 0.7811],\n",
      "        [0.8355, 0.3306, 0.3299,  ..., 0.1415, 0.0000, 0.6792],\n",
      "        ...,\n",
      "        [0.4112, 0.5724, 0.5130,  ..., 0.4694, 0.0000, 0.4159],\n",
      "        [0.6276, 0.9847, 0.6915,  ..., 0.7018, 0.0000, 0.7177],\n",
      "        [1.1395, 0.7678, 0.4531,  ..., 0.3037, 0.0000, 1.0396]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0223, 0.2643, 0.1551,  ..., 0.2163, 0.0000, 0.8628],\n",
      "        [0.5331, 1.2566, 0.7940,  ..., 0.8088, 0.0000, 0.5085],\n",
      "        [5.2506, 4.0499, 2.6137,  ..., 2.7127, 0.0000, 5.6646],\n",
      "        ...,\n",
      "        [1.0453, 0.6380, 0.2605,  ..., 0.2341, 0.0000, 0.9922],\n",
      "        [0.7170, 0.6909, 0.4036,  ..., 0.3677, 0.0000, 0.5973],\n",
      "        [0.4135, 1.1178, 0.8568,  ..., 0.6776, 0.0000, 0.6426]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5788, 1.6526, 1.0580,  ..., 1.1013, 0.0000, 1.6561],\n",
      "        [0.9400, 0.3310, 0.1811,  ..., 0.3621, 0.0000, 1.1325],\n",
      "        [5.8024, 3.1293, 2.0067,  ..., 1.6880, 0.0000, 6.2048],\n",
      "        ...,\n",
      "        [1.2171, 0.0232, 0.0547,  ..., 0.0992, 0.0000, 1.3130],\n",
      "        [1.1204, 1.0472, 0.7717,  ..., 0.8865, 0.0000, 1.2421],\n",
      "        [0.6693, 0.7701, 0.6373,  ..., 0.4536, 0.0000, 0.7209]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1763, 1.0882, 0.6093,  ..., 0.8064, 0.0000, 1.2485],\n",
      "        [0.8317, 0.5121, 0.2503,  ..., 0.3019, 0.0000, 0.9480],\n",
      "        [0.6382, 0.6532, 0.3494,  ..., 0.2139, 0.0000, 0.8305],\n",
      "        ...,\n",
      "        [1.2987, 1.0194, 0.4330,  ..., 0.5285, 0.0000, 1.7701],\n",
      "        [1.7634, 2.0325, 1.6285,  ..., 1.2751, 0.0000, 1.9266],\n",
      "        [0.7537, 0.7058, 0.5900,  ..., 0.4765, 0.0000, 0.8971]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.9482, 0.4673, 0.2328,  ..., 0.1276, 0.0000, 1.1012],\n",
      "        [0.5217, 0.3648, 0.2994,  ..., 0.2271, 0.0000, 0.6396],\n",
      "        [0.4011, 0.9720, 0.6837,  ..., 0.6636, 0.0000, 0.4048],\n",
      "        ...,\n",
      "        [0.5149, 1.0021, 0.4749,  ..., 0.4731, 0.0000, 0.6743],\n",
      "        [0.5758, 1.1527, 0.8593,  ..., 0.6313, 0.0000, 0.6560],\n",
      "        [1.4754, 0.6948, 0.5332,  ..., 0.5651, 0.0000, 1.3848]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.7450, 0.7582, 0.6397,  ..., 0.5546, 0.0000, 0.6381],\n",
      "        [0.7869, 0.5121, 0.4236,  ..., 0.1734, 0.0000, 0.8364],\n",
      "        [0.8060, 0.3734, 0.3121,  ..., 0.2865, 0.0000, 0.7653],\n",
      "        ...,\n",
      "        [0.5566, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.8705],\n",
      "        [0.4190, 0.6453, 0.5622,  ..., 0.3895, 0.0000, 0.4312],\n",
      "        [1.6577, 1.1201, 0.6350,  ..., 0.7014, 0.0000, 1.5565]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5221, 1.6085, 1.2224,  ..., 1.3262, 0.0000, 1.7496],\n",
      "        [0.8382, 0.9265, 0.6960,  ..., 0.7256, 0.0000, 0.7708],\n",
      "        [0.4126, 0.5141, 0.2581,  ..., 0.4474, 0.0000, 0.4630],\n",
      "        ...,\n",
      "        [0.4530, 0.1555, 0.0855,  ..., 0.0476, 0.0000, 0.4302],\n",
      "        [0.7266, 0.2657, 0.2605,  ..., 0.1807, 0.0000, 0.7726],\n",
      "        [0.9718, 0.5616, 0.3165,  ..., 0.3940, 0.0000, 0.7021]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0618, 0.7406, 0.6352,  ..., 0.6124, 0.0000, 1.0413],\n",
      "        [0.6221, 0.1676, 0.1192,  ..., 0.0915, 0.0000, 0.5681],\n",
      "        [0.5626, 1.1222, 0.8532,  ..., 0.7475, 0.0000, 0.5783],\n",
      "        ...,\n",
      "        [0.3597, 0.0000, 0.0670,  ..., 0.0547, 0.0000, 0.3758],\n",
      "        [1.2354, 0.2800, 0.0702,  ..., 0.2339, 0.0000, 1.2761],\n",
      "        [1.7227, 0.5618, 0.2722,  ..., 0.3561, 0.0000, 1.7864]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3729, 0.4627, 0.3505,  ..., 0.2763, 0.0000, 0.2795],\n",
      "        [1.8435, 0.3813, 0.1559,  ..., 0.1736, 0.0000, 1.8399],\n",
      "        [1.9658, 0.7483, 0.4671,  ..., 0.3671, 0.0000, 2.3183],\n",
      "        ...,\n",
      "        [1.0614, 0.7841, 0.5072,  ..., 0.5595, 0.0000, 1.0769],\n",
      "        [0.5811, 0.4667, 0.3041,  ..., 0.3437, 0.0000, 0.7922],\n",
      "        [0.7430, 0.8319, 0.4993,  ..., 0.3924, 0.0000, 0.8005]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Epoch #8\tTrain Loss: 0.235\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5997, 0.7937, 0.4725,  ..., 0.4913, 0.0000, 1.6885],\n",
      "        [1.1643, 0.4322, 0.2487,  ..., 0.2528, 0.0000, 1.3482],\n",
      "        [0.9872, 0.5427, 0.3479,  ..., 0.3384, 0.0000, 1.0507],\n",
      "        ...,\n",
      "        [1.0605, 0.0100, 0.1443,  ..., 0.1134, 0.0000, 0.9473],\n",
      "        [1.0933, 0.5167, 0.0000,  ..., 0.0000, 0.0000, 0.9335],\n",
      "        [1.9251, 1.1628, 0.6834,  ..., 0.6281, 0.0000, 1.9165]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3888, 1.0575, 0.7454,  ..., 0.8312, 0.0000, 0.5112],\n",
      "        [1.6954, 0.7012, 0.6489,  ..., 0.3950, 0.0000, 1.7700],\n",
      "        [0.7259, 1.1251, 0.8673,  ..., 0.8358, 0.0000, 0.8271],\n",
      "        ...,\n",
      "        [3.4337, 1.5602, 0.8818,  ..., 0.6789, 0.0000, 3.6240],\n",
      "        [1.9302, 0.4164, 0.2751,  ..., 0.0643, 0.0000, 1.9017],\n",
      "        [0.5214, 0.2720, 0.2067,  ..., 0.0990, 0.0000, 0.4797]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2294, 0.6094, 0.3342,  ..., 0.5256, 0.0000, 1.3217],\n",
      "        [2.6234, 2.2770, 1.3533,  ..., 0.9669, 0.0000, 2.7731],\n",
      "        [1.2046, 0.5749, 0.3935,  ..., 0.2675, 0.0000, 1.1702],\n",
      "        ...,\n",
      "        [0.6060, 0.6527, 0.4172,  ..., 0.5230, 0.0000, 0.6119],\n",
      "        [1.1030, 0.5461, 0.3632,  ..., 0.2600, 0.0000, 0.9454],\n",
      "        [1.0561, 0.2312, 0.0000,  ..., 0.0000, 0.0000, 0.9968]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2641, 0.2176, 0.1457,  ..., 0.0237, 0.0000, 1.1305],\n",
      "        [0.4760, 0.1321, 0.1142,  ..., 0.0120, 0.0000, 0.4012],\n",
      "        [1.5072, 1.0018, 0.5542,  ..., 0.4764, 0.0000, 1.8303],\n",
      "        ...,\n",
      "        [1.0363, 0.7395, 0.7313,  ..., 0.5112, 0.0000, 1.0230],\n",
      "        [1.4393, 1.2244, 0.7832,  ..., 0.6884, 0.0000, 1.8569],\n",
      "        [0.3416, 0.4044, 0.4875,  ..., 0.3882, 0.0000, 0.4565]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.5992, 0.6602, 0.5758,  ..., 0.6516, 0.0000, 0.7179],\n",
      "        [0.7336, 0.4814, 0.3103,  ..., 0.3496, 0.0000, 0.7806],\n",
      "        [0.8409, 0.3323, 0.3270,  ..., 0.1298, 0.0000, 0.6845],\n",
      "        ...,\n",
      "        [0.4104, 0.5791, 0.5131,  ..., 0.4658, 0.0000, 0.4157],\n",
      "        [0.6323, 0.9857, 0.6904,  ..., 0.6996, 0.0000, 0.7225],\n",
      "        [1.1388, 0.7696, 0.4467,  ..., 0.2952, 0.0000, 1.0391]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0191, 0.2716, 0.1578,  ..., 0.2185, 0.0000, 0.8599],\n",
      "        [0.5390, 1.2582, 0.7877,  ..., 0.8052, 0.0000, 0.5147],\n",
      "        [5.2712, 4.0491, 2.5797,  ..., 2.6677, 0.0000, 5.6855],\n",
      "        ...,\n",
      "        [1.0412, 0.6455, 0.2624,  ..., 0.2378, 0.0000, 0.9880],\n",
      "        [0.7211, 0.6942, 0.4011,  ..., 0.3638, 0.0000, 0.6017],\n",
      "        [0.4215, 1.1151, 0.8486,  ..., 0.6616, 0.0000, 0.6508]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5901, 1.6465, 1.0334,  ..., 1.0882, 0.0000, 1.6668],\n",
      "        [0.9396, 0.3363, 0.1792,  ..., 0.3614, 0.0000, 1.1323],\n",
      "        [5.8074, 3.1388, 1.9745,  ..., 1.6494, 0.0000, 6.2109],\n",
      "        ...,\n",
      "        [1.2179, 0.0263, 0.0540,  ..., 0.0984, 0.0000, 1.3139],\n",
      "        [1.1165, 1.0560, 0.7735,  ..., 0.8865, 0.0000, 1.2392],\n",
      "        [0.6737, 0.7677, 0.6282,  ..., 0.4420, 0.0000, 0.7253]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1802, 1.0892, 0.5995,  ..., 0.7947, 0.0000, 1.2528],\n",
      "        [0.8336, 0.5153, 0.2456,  ..., 0.2948, 0.0000, 0.9497],\n",
      "        [0.6389, 0.6572, 0.3513,  ..., 0.2120, 0.0000, 0.8314],\n",
      "        ...,\n",
      "        [1.2988, 1.0245, 0.4273,  ..., 0.5226, 0.0000, 1.7703],\n",
      "        [1.7752, 2.0241, 1.6103,  ..., 1.2458, 0.0000, 1.9388],\n",
      "        [0.7592, 0.7034, 0.5853,  ..., 0.4669, 0.0000, 0.9023]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.9499, 0.4693, 0.2323,  ..., 0.1242, 0.0000, 1.1027],\n",
      "        [0.5219, 0.3658, 0.2968,  ..., 0.2262, 0.0000, 0.6395],\n",
      "        [0.4015, 0.9765, 0.6831,  ..., 0.6654, 0.0000, 0.4055],\n",
      "        ...,\n",
      "        [0.5171, 1.0025, 0.4699,  ..., 0.4621, 0.0000, 0.6761],\n",
      "        [0.5719, 1.1608, 0.8654,  ..., 0.6330, 0.0000, 0.6526],\n",
      "        [1.4754, 0.6984, 0.5310,  ..., 0.5622, 0.0000, 1.3840]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.7450, 0.7637, 0.6391,  ..., 0.5565, 0.0000, 0.6381],\n",
      "        [0.7903, 0.5121, 0.4184,  ..., 0.1659, 0.0000, 0.8394],\n",
      "        [0.8039, 0.3752, 0.3095,  ..., 0.2804, 0.0000, 0.7631],\n",
      "        ...,\n",
      "        [0.5445, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.8618],\n",
      "        [0.4203, 0.6470, 0.5593,  ..., 0.3892, 0.0000, 0.4323],\n",
      "        [1.6593, 1.1248, 0.6335,  ..., 0.7001, 0.0000, 1.5583]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5124, 1.6195, 1.2181,  ..., 1.3343, 0.0000, 1.7387],\n",
      "        [0.8402, 0.9257, 0.6879,  ..., 0.7192, 0.0000, 0.7731],\n",
      "        [0.4102, 0.5175, 0.2610,  ..., 0.4478, 0.0000, 0.4605],\n",
      "        ...,\n",
      "        [0.4535, 0.1568, 0.0837,  ..., 0.0460, 0.0000, 0.4308],\n",
      "        [0.7271, 0.2671, 0.2587,  ..., 0.1781, 0.0000, 0.7727],\n",
      "        [0.9713, 0.5663, 0.3175,  ..., 0.3952, 0.0000, 0.7013]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0584e+00, 7.4592e-01, 6.3262e-01,  ..., 5.9908e-01, 0.0000e+00,\n",
      "         1.0382e+00],\n",
      "        [6.2487e-01, 1.6883e-01, 1.1878e-01,  ..., 9.0537e-02, 0.0000e+00,\n",
      "         5.7054e-01],\n",
      "        [5.6411e-01, 1.1246e+00, 8.5150e-01,  ..., 7.4845e-01, 0.0000e+00,\n",
      "         5.7965e-01],\n",
      "        ...,\n",
      "        [3.5978e-01, 2.4778e-04, 6.8236e-02,  ..., 5.3103e-02, 0.0000e+00,\n",
      "         3.7570e-01],\n",
      "        [1.2347e+00, 2.8267e-01, 6.3977e-02,  ..., 2.3579e-01, 0.0000e+00,\n",
      "         1.2755e+00],\n",
      "        [1.7226e+00, 5.6818e-01, 2.7268e-01,  ..., 3.5611e-01, 0.0000e+00,\n",
      "         1.7854e+00]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3725, 0.4664, 0.3498,  ..., 0.2788, 0.0000, 0.2792],\n",
      "        [1.8474, 0.3798, 0.1459,  ..., 0.1612, 0.0000, 1.8436],\n",
      "        [1.9667, 0.7524, 0.4621,  ..., 0.3557, 0.0000, 2.3195],\n",
      "        ...,\n",
      "        [1.0622, 0.7825, 0.4997,  ..., 0.5537, 0.0000, 1.0772],\n",
      "        [0.5822, 0.4662, 0.2983,  ..., 0.3403, 0.0000, 0.7927],\n",
      "        [0.7463, 0.8308, 0.4935,  ..., 0.3865, 0.0000, 0.8037]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Epoch #9\tTrain Loss: 0.233\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5966, 0.7964, 0.4609,  ..., 0.4841, 0.0000, 1.6850],\n",
      "        [1.1664, 0.4319, 0.2407,  ..., 0.2479, 0.0000, 1.3502],\n",
      "        [0.9847, 0.5473, 0.3467,  ..., 0.3367, 0.0000, 1.0475],\n",
      "        ...,\n",
      "        [1.0610, 0.0179, 0.1461,  ..., 0.1153, 0.0000, 0.9477],\n",
      "        [1.0944, 0.5187, 0.0000,  ..., 0.0000, 0.0000, 0.9350],\n",
      "        [1.9268, 1.1658, 0.6791,  ..., 0.6240, 0.0000, 1.9183]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3889, 1.0604, 0.7429,  ..., 0.8322, 0.0000, 0.5115],\n",
      "        [1.6961, 0.7023, 0.6385,  ..., 0.3863, 0.0000, 1.7704],\n",
      "        [0.7263, 1.1306, 0.8688,  ..., 0.8372, 0.0000, 0.8277],\n",
      "        ...,\n",
      "        [3.4374, 1.5628, 0.8633,  ..., 0.6651, 0.0000, 3.6276],\n",
      "        [1.9280, 0.4216, 0.2759,  ..., 0.0608, 0.0000, 1.8994],\n",
      "        [0.5217, 0.2749, 0.2083,  ..., 0.0981, 0.0000, 0.4800]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2296, 0.6132, 0.3319,  ..., 0.5234, 0.0000, 1.3215],\n",
      "        [2.6284, 2.2746, 1.3369,  ..., 0.9454, 0.0000, 2.7779],\n",
      "        [1.2082, 0.5749, 0.3782,  ..., 0.2635, 0.0000, 1.1736],\n",
      "        ...,\n",
      "        [0.6095, 0.6527, 0.4132,  ..., 0.5185, 0.0000, 0.6153],\n",
      "        [1.1039, 0.5484, 0.3561,  ..., 0.2574, 0.0000, 0.9461],\n",
      "        [1.0572, 0.2348, 0.0000,  ..., 0.0000, 0.0000, 0.9973]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.2650, 0.2216, 0.1462,  ..., 0.0202, 0.0000, 1.1314],\n",
      "        [0.4765, 0.1337, 0.1143,  ..., 0.0124, 0.0000, 0.4017],\n",
      "        [1.5067, 1.0070, 0.5499,  ..., 0.4716, 0.0000, 1.8293],\n",
      "        ...,\n",
      "        [1.0341, 0.7417, 0.7311,  ..., 0.5090, 0.0000, 1.0208],\n",
      "        [1.4396, 1.2297, 0.7831,  ..., 0.6874, 0.0000, 1.8571],\n",
      "        [0.3439, 0.4019, 0.4835,  ..., 0.3836, 0.0000, 0.4585]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.5998, 0.6621, 0.5745,  ..., 0.6489, 0.0000, 0.7187],\n",
      "        [0.7350, 0.4823, 0.3041,  ..., 0.3466, 0.0000, 0.7820],\n",
      "        [0.8458, 0.3327, 0.3232,  ..., 0.1173, 0.0000, 0.6890],\n",
      "        ...,\n",
      "        [0.4127, 0.5819, 0.5093,  ..., 0.4582, 0.0000, 0.4183],\n",
      "        [0.6358, 0.9864, 0.6893,  ..., 0.6973, 0.0000, 0.7259],\n",
      "        [1.1378, 0.7704, 0.4392,  ..., 0.2860, 0.0000, 1.0380]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0164, 0.2771, 0.1592,  ..., 0.2190, 0.0000, 0.8572],\n",
      "        [0.5431, 1.2598, 0.7817,  ..., 0.8020, 0.0000, 0.5190],\n",
      "        [5.2878, 4.0446, 2.5445,  ..., 2.6211, 0.0000, 5.7015],\n",
      "        ...,\n",
      "        [1.0391, 0.6498, 0.2612,  ..., 0.2382, 0.0000, 0.9853],\n",
      "        [0.7253, 0.6961, 0.3973,  ..., 0.3582, 0.0000, 0.6060],\n",
      "        [0.4273, 1.1132, 0.8418,  ..., 0.6475, 0.0000, 0.6565]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5952, 1.6433, 1.0127,  ..., 1.0782, 0.0000, 1.6714],\n",
      "        [0.9395, 0.3399, 0.1760,  ..., 0.3594, 0.0000, 1.1322],\n",
      "        [5.8071, 3.1458, 1.9426,  ..., 1.6112, 0.0000, 6.2102],\n",
      "        ...,\n",
      "        [1.2173, 0.0295, 0.0536,  ..., 0.0975, 0.0000, 1.3132],\n",
      "        [1.1131, 1.0628, 0.7734,  ..., 0.8847, 0.0000, 1.2361],\n",
      "        [0.6764, 0.7655, 0.6196,  ..., 0.4313, 0.0000, 0.7281]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.1829, 1.0893, 0.5894,  ..., 0.7828, 0.0000, 1.2558],\n",
      "        [0.8360, 0.5167, 0.2391,  ..., 0.2863, 0.0000, 0.9519],\n",
      "        [0.6395, 0.6599, 0.3523,  ..., 0.2090, 0.0000, 0.8320],\n",
      "        ...,\n",
      "        [1.3004, 1.0259, 0.4185,  ..., 0.5136, 0.0000, 1.7718],\n",
      "        [1.7830, 2.0169, 1.5941,  ..., 1.2191, 0.0000, 1.9466],\n",
      "        [0.7639, 0.7002, 0.5802,  ..., 0.4570, 0.0000, 0.9064]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.9507, 0.4706, 0.2314,  ..., 0.1208, 0.0000, 1.1031],\n",
      "        [0.5219, 0.3663, 0.2939,  ..., 0.2253, 0.0000, 0.6392],\n",
      "        [0.4040, 0.9775, 0.6792,  ..., 0.6635, 0.0000, 0.4081],\n",
      "        ...,\n",
      "        [0.5179, 1.0030, 0.4654,  ..., 0.4520, 0.0000, 0.6765],\n",
      "        [0.5707, 1.1646, 0.8672,  ..., 0.6294, 0.0000, 0.6516],\n",
      "        [1.4753, 0.6999, 0.5269,  ..., 0.5574, 0.0000, 1.3828]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.7458, 0.7670, 0.6368,  ..., 0.5561, 0.0000, 0.6388],\n",
      "        [0.7939, 0.5100, 0.4110,  ..., 0.1561, 0.0000, 0.8428],\n",
      "        [0.8029, 0.3752, 0.3052,  ..., 0.2721, 0.0000, 0.7618],\n",
      "        ...,\n",
      "        [0.5360, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.8542],\n",
      "        [0.4227, 0.6463, 0.5544,  ..., 0.3867, 0.0000, 0.4345],\n",
      "        [1.6638, 1.1242, 0.6270,  ..., 0.6931, 0.0000, 1.5625]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.5046, 1.6258, 1.2099,  ..., 1.3385, 0.0000, 1.7298],\n",
      "        [0.8435, 0.9220, 0.6775,  ..., 0.7104, 0.0000, 0.7766],\n",
      "        [0.4094, 0.5188, 0.2616,  ..., 0.4460, 0.0000, 0.4594],\n",
      "        ...,\n",
      "        [0.4536, 0.1578, 0.0819,  ..., 0.0445, 0.0000, 0.4308],\n",
      "        [0.7270, 0.2680, 0.2564,  ..., 0.1752, 0.0000, 0.7722],\n",
      "        [0.9716, 0.5692, 0.3163,  ..., 0.3945, 0.0000, 0.7013]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[1.0568, 0.7480, 0.6271,  ..., 0.5843, 0.0000, 1.0366],\n",
      "        [0.6277, 0.1694, 0.1178,  ..., 0.0889, 0.0000, 0.5730],\n",
      "        [0.5649, 1.1262, 0.8492,  ..., 0.7482, 0.0000, 0.5802],\n",
      "        ...,\n",
      "        [0.3596, 0.0024, 0.0694,  ..., 0.0512, 0.0000, 0.3753],\n",
      "        [1.2324, 0.2856, 0.0580,  ..., 0.2384, 0.0000, 1.2733],\n",
      "        [1.7211, 0.5741, 0.2734,  ..., 0.3560, 0.0000, 1.7831]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[0.3721, 0.4697, 0.3487,  ..., 0.2807, 0.0000, 0.2789],\n",
      "        [1.8483, 0.3797, 0.1380,  ..., 0.1507, 0.0000, 1.8442],\n",
      "        [1.9671, 0.7552, 0.4563,  ..., 0.3430, 0.0000, 2.3201],\n",
      "        ...,\n",
      "        [1.0616, 0.7811, 0.4925,  ..., 0.5483, 0.0000, 1.0762],\n",
      "        [0.5819, 0.4661, 0.2934,  ..., 0.3374, 0.0000, 0.7919],\n",
      "        [0.7500, 0.8281, 0.4861,  ..., 0.3780, 0.0000, 0.8073]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Epoch #10\tTrain Loss: 0.231\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "model.train()\n",
    "train_losses=[]\n",
    "epochs=10\n",
    "for epoch in range(epochs):\n",
    "    progress_bar = tqdm_notebook(train_loader, leave=False)\n",
    "    losses = []\n",
    "    total = 0\n",
    "    for inputs, target in progress_bar:\n",
    "        model.zero_grad()\n",
    "\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output.squeeze(), target.float())\n",
    "        \n",
    "        loss.backward()\n",
    "              \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "\n",
    "        optim.step()\n",
    "        \n",
    "        progress_bar.set_description(f'Loss: {loss.item():.3f}')\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        total += 1\n",
    "    \n",
    "    epoch_loss = sum(losses) / total\n",
    "    train_losses.append(epoch_loss)\n",
    "        \n",
    "    tqdm.write(f'Epoch #{epoch + 1}\\tTrain Loss: {epoch_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_vector = torch.LongTensor(data.cv.transform([text]).toarray())\n",
    "\n",
    "        output = model(test_vector)\n",
    "        prediction = torch.sigmoid(output).item()\n",
    "\n",
    "        if prediction > 0.5:\n",
    "            print(f'{prediction:0.3}: Positive sentiment')\n",
    "        else:\n",
    "            print(f'{prediction:0.3}: Negative sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1510, 0.0074, 0.0059, 0.0824, 0.0444, 0.1067, 0.1020, 0.0891, 0.1467,\n",
      "         0.1010, 0.1321, 0.0000, 0.0000, 0.0136, 0.0499, 0.1098, 0.0686, 0.0613,\n",
      "         0.1042, 0.0000, 0.0000, 0.0402, 0.1098, 0.0778, 0.1040, 0.0826, 0.0822,\n",
      "         0.0865, 0.0775, 0.1040, 0.0105, 0.0989, 0.0000, 0.1581, 0.0493, 0.1019,\n",
      "         0.0976, 0.0901, 0.0996, 0.0175, 0.1666, 0.0000, 0.0000, 0.0157, 0.0902,\n",
      "         0.0058, 0.1340, 0.0316, 0.0000, 0.0000, 0.1825, 0.1172, 0.0780, 0.0706,\n",
      "         0.0000, 0.0000, 0.1323, 0.0000, 0.1248, 0.0922, 0.1171, 0.0655, 0.1589,\n",
      "         0.0000, 0.0968, 0.0000, 0.0000, 0.0264, 0.0000, 0.0264, 0.1493, 0.0000,\n",
      "         0.1545, 0.0304, 0.1395, 0.0676, 0.0978, 0.0071, 0.0831, 0.1261, 0.0260,\n",
      "         0.2259, 0.1405, 0.1551, 0.0244, 0.0021, 0.0000, 0.0730, 0.1074, 0.0870,\n",
      "         0.1294, 0.0735, 0.0000, 0.0275, 0.1656, 0.0032, 0.0081, 0.1868, 0.0000,\n",
      "         0.1410, 0.1119, 0.0000, 0.0182, 0.1118, 0.0852, 0.0036, 0.0000, 0.1206,\n",
      "         0.0454, 0.0000, 0.0000, 0.0805, 0.0543, 0.0278, 0.0000, 0.2027, 0.1084,\n",
      "         0.0616, 0.1154, 0.1179, 0.0066, 0.0745, 0.0000, 0.1109, 0.0920, 0.0000,\n",
      "         0.0000, 0.1201]])\n",
      "0.7: Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "#predicting the sentence\n",
    "test_text='It was a wonderful thing to watch! I would to love to see it again!!'\n",
    "predict(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
